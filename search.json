[
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-1-import-pandas-and-load-the-data",
    "href": "notebooks/Exploration_02.html#part-1-import-pandas-and-load-the-data",
    "title": "Data Exploration 02",
    "section": "Part 1: Import Pandas and load the data",
    "text": "Part 1: Import Pandas and load the data\nRemember to import Pandas the conventional way. If you‚Äôve forgotten how, you may want to review Data Exploration 01.\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce you‚Äôve loaded the data, it‚Äôs a good idea to poke around a little bit to find out what you‚Äôre dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 1: Enter your code below to import Pandas according to the\n# conventional method. Then load the dataset into a Pandas dataframe.\nimport pandas as pd\n# Write any code needed to explore the data by seeing what the first few\n# rows look like. Then display a technical summary of the data to determine\n# the data types of each column, and which columns have missing data.\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\")\n\n\ndf\n\n\n    \n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n72\nTriples\nG\nC\n110\n2\n1\n250\n0.0\n21.0\n3\n60\n25\n3\n1.0\n0.75\n39.106174\n\n\n73\nTrix\nG\nC\n110\n1\n1\n140\n0.0\n13.0\n12\n25\n25\n2\n1.0\n1.00\n27.753301\n\n\n74\nWheat Chex\nR\nC\n100\n3\n1\n230\n3.0\n17.0\n3\n115\n25\n1\n1.0\n0.67\n49.787445\n\n\n75\nWheaties\nG\nC\n100\n3\n1\n200\n3.0\n17.0\n3\n110\n25\n1\n1.0\n1.00\n51.592193\n\n\n76\nWheaties Honey Gold\nG\nC\n110\n2\n1\n200\n1.0\n16.0\n8\n60\n25\n1\n1.0\n0.75\n36.187559\n\n\n\n\n77 rows √ó 16 columns"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-2-calculate-summary-statistics",
    "href": "notebooks/Exploration_02.html#part-2-calculate-summary-statistics",
    "title": "Data Exploration 02",
    "section": "Part 2: Calculate Summary Statistics",
    "text": "Part 2: Calculate Summary Statistics\nThe marketing team has determined that when choosing a cereal, consumers are most interested in calories, sugars, fiber, fat, and protein.\nFirst, let‚Äôs calcuate some summary statistics for these categories across the entire dataset. We‚Äôre particularly intrested in the mean, median, standard deviation, min, and max values.\nThere are multiple ways to accomplish this.\n\n# Part 2: Enter your code below to calculate summary statistics for the\n# calories, sugars, fiber, fat, and protein features.\n\ndf[['calories', 'sugars', 'fiber', 'fat', 'protein']].describe()\n\n\n    \n\n\n\n\n\n\ncalories\nsugars\nfiber\nfat\nprotein\n\n\n\n\ncount\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n\n\nmean\n106.883117\n6.922078\n2.151948\n1.012987\n2.545455\n\n\nstd\n19.484119\n4.444885\n2.383364\n1.006473\n1.094790\n\n\nmin\n50.000000\n-1.000000\n0.000000\n0.000000\n1.000000\n\n\n25%\n100.000000\n3.000000\n1.000000\n0.000000\n2.000000\n\n\n50%\n110.000000\n7.000000\n2.000000\n1.000000\n3.000000\n\n\n75%\n110.000000\n11.000000\n3.000000\n2.000000\n3.000000\n\n\nmax\n160.000000\n15.000000\n14.000000\n5.000000\n6.000000"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-3-transform-data",
    "href": "notebooks/Exploration_02.html#part-3-transform-data",
    "title": "Data Exploration 02",
    "section": "Part 3: Transform Data",
    "text": "Part 3: Transform Data\nTo make analysis easier, you want to convert the manufacturer codes used in the dataset to the manufacturer names.\nFirst, display the count of each manufacturer code value used in the dataset (found in the mfr column).\nThen, create a new column with the appropriate manufacturer name for each entry, using this mapping:\nA = American Home Food Products\nG = General Mills\nK = Kelloggs\nN = Nabisco\nP = Post\nQ = Quaker Oats\nR = Ralston Purina\n\nNote: While the tutorial linked above uses the replace function, using the map function instead can often be much faster and more memory efficient, especially for large datasets.\n\n\n# Display the count of values for the manufacturer code (\"mfr\" column), then\n# create a new column containing the appropriate manufacturer names.\ndf['mfr'].value_counts()\nmfr_map = {\n    \"A\": \"American Home Food Products\",\n    \"G\": \"General Mills\",\n    \"K\": \"Kelloggs\",\n    \"N\": \"Nabisco\",\n    \"P\": \"Post\",\n    \"Q\": \"Quaker Oats\",\n    \"R\": \"Ralston Purina\"\n}\ndf[\"manufacturer\"] = df[\"mfr\"].map(mfr_map)\ndf[[\"mfr\", \"manufacturer\"]]\n\n\n    \n\n\n\n\n\n\nmfr\nmanufacturer\n\n\n\n\n0\nN\nNabisco\n\n\n1\nQ\nQuaker Oats\n\n\n2\nK\nKelloggs\n\n\n3\nK\nKelloggs\n\n\n4\nR\nRalston Purina\n\n\n...\n...\n...\n\n\n72\nG\nGeneral Mills\n\n\n73\nG\nGeneral Mills\n\n\n74\nR\nRalston Purina\n\n\n75\nG\nGeneral Mills\n\n\n76\nG\nGeneral Mills\n\n\n\n\n77 rows √ó 2 columns"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-4-visualization",
    "href": "notebooks/Exploration_02.html#part-4-visualization",
    "title": "Data Exploration 02",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nLet‚Äôs do some more data exploration visually.\nImport your visualization library of choice and set any needed configuration options.\n\n# Import your visualization library\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"darkgrid\")\n\n\nSugar Distribution\nMarketing tells us that their surveys have revealed that sugar content is the number one concern of consumers when choosing cereal.\nThey would like to see the following visualizations:\n\nA histogram plot of the sugar content in all cereals.\nA scatter plot showing the relationship between sugar and calories.\nA box plot showing the distribution of sugar content by manufacturer.\n\n\n# Create the three visualzations requested by the the marketing team\n# Chart 1\nsns.histplot(data=df, x=\"sugars\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Chart 2\nsns.scatterplot(\n    data = df,\n    x = \"calories\",\n    y = \"sugars\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Chart 3\nsns.boxplot(data = df, x = \"manufacturer\",y = \"sugars\")\nplt.xticks(rotation = 45)\nplt.show()"
  },
  {
    "objectID": "notebooks/Exploration_02.html#above-and-beyond",
    "href": "notebooks/Exploration_02.html#above-and-beyond",
    "title": "Data Exploration 02",
    "section": "üåü Above and Beyond üåü",
    "text": "üåü Above and Beyond üåü\nThe marketing team is pleased with what you‚Äôve accomplished so far. They have a meeting with top cereal executives in the morning, and they‚Äôd like you to do as many of the following additional tasks as you have time for:\n\nWeight Watchers used to have an older points system that used this formula: (calories / 50) + (fat / 12) - (fiber / 5), but only the first 4 grams of fiber were included in the calculation. For comparison‚Äôs sake, create an additional column with the calculation for the old points system.\nMarketing really likes the boxplot of the sugar content for each cereal, they‚Äôd like similar plots for calories and fat, but using different color schemes for each chart.\n\n\n# Above and Beyond Task 1\nfiber_cap = df['fiber'].clip(upper=4)\ndf['old_ww_points'] = ((df['calories']/50) + (df['fat']/12) - (fiber_cap/5)).round().astype(int)\ndf\n\n\n    \n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\nmanufacturer\nww_points\nold_ww_points\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\nNabisco\n4\n1\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\nQuaker Oats\n6\n2\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\nKelloggs\n3\n1\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\nKelloggs\n2\n0\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843\nRalston Purina\n5\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n72\nTriples\nG\nC\n110\n2\n1\n250\n0.0\n21.0\n3\n60\n25\n3\n1.0\n0.75\n39.106174\nGeneral Mills\n4\n2\n\n\n73\nTrix\nG\nC\n110\n1\n1\n140\n0.0\n13.0\n12\n25\n25\n2\n1.0\n1.00\n27.753301\nGeneral Mills\n5\n2\n\n\n74\nWheat Chex\nR\nC\n100\n3\n1\n230\n3.0\n17.0\n3\n115\n25\n1\n1.0\n0.67\n49.787445\nRalston Purina\n4\n1\n\n\n75\nWheaties\nG\nC\n100\n3\n1\n200\n3.0\n17.0\n3\n110\n25\n1\n1.0\n1.00\n51.592193\nGeneral Mills\n4\n1\n\n\n76\nWheaties Honey Gold\nG\nC\n110\n2\n1\n200\n1.0\n16.0\n8\n60\n25\n1\n1.0\n0.75\n36.187559\nGeneral Mills\n5\n2\n\n\n\n\n77 rows √ó 19 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Above and Beyond Task 2.1 : Calories\nsns.boxplot(data = df, x = \"manufacturer\", y = \"calories\", color = \"red\")\nplt.xticks(rotation = 45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Above and Beyond Task 2.2 : Fat\nsns.boxplot(data = df, x = \"manufacturer\",y = \"fat\", color = \"green\")\nplt.xticks(rotation = 45)\nplt.show()"
  },
  {
    "objectID": "notebooks/Exploration_01.html",
    "href": "notebooks/Exploration_01.html",
    "title": "Data Exploration 01",
    "section": "",
    "text": "A consumer watchdog group wants to see if Netflix has more movies for adults or children.\nUsing a dataset containing metadata for all of the movies Netflix had available on their platform in 2019, we‚Äôll use the MPAA movie rating system to determine if they are correct."
  },
  {
    "objectID": "notebooks/Exploration_01.html#mpaa-movie-ratings",
    "href": "notebooks/Exploration_01.html#mpaa-movie-ratings",
    "title": "Data Exploration 01",
    "section": "MPAA Movie Ratings:",
    "text": "MPAA Movie Ratings:\n\nG: All ages admitted.\nPG: Some material may not be suitable for children.\nPG-13: Some material may be inappropriate for children under 13.\nR: Under 17 requires accompanying parent or adult guardian\nNC-17: No One 17 and Under Admitted\n\nMost people would consider G and PG as ratings suitable for children. However, not everyone would agree that a PG-13 movie is necssarily a children‚Äôs movie. It is up to you to decide how to handle this."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-1-import-pandas",
    "href": "notebooks/Exploration_01.html#part-1-import-pandas",
    "title": "Data Exploration 01",
    "section": "Part 1: Import Pandas",
    "text": "Part 1: Import Pandas\nThe pandas library is a python library used for data analysis and manipulation. It will provide the core functionality for most of what you do in the data exploration and preprocessing stages of most machine learning projects.\nPlease see this Getting Started Guide for information on the conventional way to import Pandas into your project, as well as other helpful tips for common Pandas tasks.\n\n# Part 1: Enter the code below to import Pandas according to the\n# conventional method.\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-2-load-the-data",
    "href": "notebooks/Exploration_01.html#part-2-load-the-data",
    "title": "Data Exploration 01",
    "section": "Part 2: Load the data",
    "text": "Part 2: Load the data\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce you‚Äôve loaded the data, it‚Äôs a good idea to poke around a little bit to find out what you‚Äôre dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 2: Load the dataset into a Pandas dataframe.\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\")\n\n\n# Then, explore the data by seeing what the first few rows look like.\ndf.head()\n\n\n    \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n0\n81145628\nMovie\nNorm of the North: King Sized Adventure\nRichard Finn, Tim Maltby\nAlan Marriott, Andrew Toth, Brian Dobson, Cole...\nUnited States, India, South Korea, China\nSeptember 9, 2019\n2019\nTV-PG\n90 min\nChildren & Family Movies, Comedies\nBefore planning an awesome wedding for his gra...\n\n\n1\n80117401\nMovie\nJandino: Whatever it Takes\nNaN\nJandino Asporaat\nUnited Kingdom\nSeptember 9, 2016\n2016\nTV-MA\n94 min\nStand-Up Comedy\nJandino Asporaat riffs on the challenges of ra...\n\n\n2\n70234439\nTV Show\nTransformers Prime\nNaN\nPeter Cullen, Sumalee Montano, Frank Welker, J...\nUnited States\nSeptember 8, 2018\n2013\nTV-Y7-FV\n1 Season\nKids' TV\nWith the help of three human allies, the Autob...\n\n\n3\n80058654\nTV Show\nTransformers: Robots in Disguise\nNaN\nWill Friedle, Darren Criss, Constance Zimmer, ...\nUnited States\nSeptember 8, 2018\n2016\nTV-Y7\n1 Season\nKids' TV\nWhen a prison ship crash unleashes hundreds of...\n\n\n4\n80125979\nMovie\n#realityhigh\nFernando Lebrija\nNesta Cooper, Kate Walsh, John Michael Higgins...\nUnited States\nSeptember 8, 2017\n2017\nTV-14\n99 min\nComedies\nWhen nerdy high schooler Dani finally attracts...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n# Next, display a technical summary of the data to determine the data types of each column, and which columns have missing data.\n\ndf.describe()\n\n\n    \n\n\n\n\n\n\nshow_id\nrelease_year\n\n\n\n\ncount\n6.234000e+03\n6234.00000\n\n\nmean\n7.670368e+07\n2013.35932\n\n\nstd\n1.094296e+07\n8.81162\n\n\nmin\n2.477470e+05\n1925.00000\n\n\n25%\n8.003580e+07\n2013.00000\n\n\n50%\n8.016337e+07\n2016.00000\n\n\n75%\n8.024489e+07\n2018.00000\n\n\nmax\n8.123573e+07\n2020.00000"
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-3-filter-the-data",
    "href": "notebooks/Exploration_01.html#part-3-filter-the-data",
    "title": "Data Exploration 01",
    "section": "Part 3: Filter the Data",
    "text": "Part 3: Filter the Data\nSince we‚Äôre just interested in movies, we‚Äôll need to filter out anything that isn‚Äôt a movie for our analysis. The type feature contains this information.\nOnce we have the subset, we should see how many rows it contains. There are a variety of ways to get the length of a data frame.\n\n# Use pandas's filtering abilitites to select the subset of data\n# that represents movies, then calculate how many rows are in the filtered data.\ndf_movies = df[df[\"type\"] == \"Movie\"]\ndf_movies\n\n\n    \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n0\n81145628\nMovie\nNorm of the North: King Sized Adventure\nRichard Finn, Tim Maltby\nAlan Marriott, Andrew Toth, Brian Dobson, Cole...\nUnited States, India, South Korea, China\nSeptember 9, 2019\n2019\nTV-PG\n90 min\nChildren & Family Movies, Comedies\nBefore planning an awesome wedding for his gra...\n\n\n1\n80117401\nMovie\nJandino: Whatever it Takes\nNaN\nJandino Asporaat\nUnited Kingdom\nSeptember 9, 2016\n2016\nTV-MA\n94 min\nStand-Up Comedy\nJandino Asporaat riffs on the challenges of ra...\n\n\n4\n80125979\nMovie\n#realityhigh\nFernando Lebrija\nNesta Cooper, Kate Walsh, John Michael Higgins...\nUnited States\nSeptember 8, 2017\n2017\nTV-14\n99 min\nComedies\nWhen nerdy high schooler Dani finally attracts...\n\n\n6\n70304989\nMovie\nAutomata\nGabe Ib√°√±ez\nAntonio Banderas, Dylan McDermott, Melanie Gri...\nBulgaria, United States, Spain, Canada\nSeptember 8, 2017\n2014\nR\n110 min\nInternational Movies, Sci-Fi & Fantasy, Thrillers\nIn a dystopian future, an insurance adjuster f...\n\n\n7\n80164077\nMovie\nFabrizio Copano: Solo pienso en mi\nRodrigo Toro, Francisco Schultz\nFabrizio Copano\nChile\nSeptember 8, 2017\n2017\nTV-MA\n60 min\nStand-Up Comedy\nFabrizio Copano takes audience participation t...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5577\n80085438\nMovie\nFrank and Cindy\nG.J. Echternkamp\nNaN\nUnited States\nApril 1, 2016\n2007\nTV-MA\n70 min\nDocumentaries\nFrank was a rising pop star when he married Ci...\n\n\n5578\n80085439\nMovie\nFrank and Cindy\nG.J. Echternkamp\nRene Russo, Oliver Platt, Johnny Simmons, Jane...\nUnited States\nApril 1, 2016\n2015\nR\n102 min\nComedies, Dramas, Independent Movies\nA student filmmaker vengefully turns his camer...\n\n\n5579\n80011846\nMovie\nIverson\nZatella Beatty\nAllen Iverson\nUnited States\nApril 1, 2016\n2014\nNR\n88 min\nDocumentaries, Sports Movies\nThis unfiltered documentary follows the rocky ...\n\n\n5580\n80064521\nMovie\nJeremy Scott: The People's Designer\nVlad Yudin\nJeremy Scott\nUnited States\nApril 1, 2016\n2015\nPG-13\n109 min\nDocumentaries\nThe journey of fashion designer Jeremy Scott f...\n\n\n6231\n80116008\nMovie\nLittle Baby Bum: Nursery Rhyme Friends\nNaN\nNaN\nNaN\nNaN\n2016\nNaN\n60 min\nMovies\nNursery rhymes and original music for children...\n\n\n\n\n4265 rows √ó 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nMPAA Ratings\nNow that we have only movies, let‚Äôs get a quick count of the values being used in the rating feature.\n\n# Determine the number of records for each value of the \"rating\" feature.\n# Remember to count the values in your subset only, not in the original dataframe.\ndf_movies[\"rating\"].unique()\ndf_movies[\"rating\"].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nrating\n\n\n\n\n\nTV-MA\n1348\n\n\nTV-14\n1038\n\n\nR\n506\n\n\nTV-PG\n432\n\n\nPG-13\n286\n\n\nNR\n202\n\n\nPG\n183\n\n\nTV-G\n80\n\n\nTV-Y7\n69\n\n\nTV-Y\n41\n\n\nG\n36\n\n\nTV-Y7-FV\n27\n\n\nUR\n7\n\n\nNC-17\n2\n\n\n\n\ndtype: int64\n\n\n\n\nMore Filtering\nThere are apparently some ‚Äúmade for TV‚Äù movies in the list that don‚Äôt fit the MPAA rating scheme.\nLet‚Äôs filter some more to just see movies rated with the standard MPAA ratings of G, PG, PG-13, R, and NC-17.\n\n# Filter the list of movies to select a new subset containing only movies with\n# a standard MPAA rating. Calculate how many rows are in this new set, and\n# then see which ratings appear most often.\nmovie_ratings = [\"G\", \"PG\", \"PG-13\", \"R\", \"NC-17\"]\ndf_filtered = df_movies[df_movies[\"rating\"].isin(movie_ratings)]\ndf_filtered.head()\ndf_filtered[\"rating\"].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nrating\n\n\n\n\n\nR\n506\n\n\nPG-13\n286\n\n\nPG\n183\n\n\nG\n36\n\n\nNC-17\n2\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-4-visualization",
    "href": "notebooks/Exploration_01.html#part-4-visualization",
    "title": "Data Exploration 01",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nNow that we have explored and preprocessed our data, let‚Äôs create a visualization to summarize our findings.\n\nExploration vs Presentation\nBroadly speaking, there are two types of visualizations: * Barebones visualizations you might use to get a quick, visual understanding of the data while you‚Äôre trying to decide how it all fits together. * Presentation-quality visualizations that you would include in a report or presentation for management or other stakeholders.\n\n\nVisualization Tools\nThere are many different visualization tools availble. In the sections below, we‚Äôll explore the three most common. Each of these libraries has strengths and weaknesses.\nIt is probably a good idea for you to become familiar with each one, and then become proficient at whichever one you like the best.\n\n\nAltair\nThe Altair visualization library provides a large variety of very easy to use statistical charting tools.\nAltair uses a declarative language to build up charts piece by piece.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nalt.Chart(employees).mark_boxplot().encode(\n    x='job',\n    y='salary'\n)\n\n# Make a box plot style categorical plot, and customize the results\nalt.Chart(employees).mark_boxplot().encode(\n    alt.X('job', title='Job title'),\n    alt.Y('salary', title='Annual salary in thousands of $USD')\n).properties(\n  title='Salaries by Job Title'\n)\nLike with Pandas, there is a conventional way to import Altair into your projects.\n\n# Import the Altair library the conventional way.\nimport altair as alt\n\nLet‚Äôs create a barchart showing the count of each movie rating by using Altair‚Äôs aggregation capabilities.\nIn this example, we see the x axis being set to a feature called a, and the y axis set to the average() of a feature called b.\nIn our case, we want the x axis to be set to rating and the y axis to be the count() of rating.\n\n# Use Altair to create a bar chart comparing the count of each movie rating\nalt.Chart(df_filtered).mark_bar().encode(\n    alt.X('rating').title(\"Movie Ratings\"),\n    alt.Y('count(rating)').title(\"Count of Movies\")\n)\n\n\n\n\n\n\n\n\n\nSeaborn\nWhile Altair uses a ‚Äúdeclarative‚Äù syntax for building charts piece by piece, the Seaborn library provides a large variety of pre-made charts for common statistical needs.\nThese charts are divided into different categories. Each category has a high-level interface you can use for simplicity, and then a specific function for each chart that you can use if you need more control over how the chart looks.\nSeaborn uses matplotlib for its drawing, and the chart-specific functions each return a matplitlib axes object if you need additional customization.\nFor example, there are several different types of categorical plots in seaborn: bar plots, box plots, point plots, count plots, swarm plots, etc‚Ä¶\nEach of these plots can be accessed using the catplot function.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nsns.catplot(data=employees, x='job', y='salary', kind='box')\n\n# Make a swarm plot style categorical plot\nsns.catplot(data=employees, x='job', y='salary', kind='swarm')\nAlternatively, you can use the plot specific functions to give yourself more control over the output by using matplotlib functions:\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = sns.boxplot(data=employees, x='job', y='salary')\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\nLike with Pandas, there is a conventional way to import Seaborn into your projects.\nOptionally, you may wish to set some default chart aesthetics by setting the chart style.\n\n# Import the seaborn library the conventional way. Then optionally configure\n# the default chart style.\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\nSince the rating column uses categorical data, we need to use Seaborn‚Äôs categorical visualizations.\nIn particular, we want a ‚Äúcount plot‚Äù that will display a count of movie ratings.\n\n# Use seaborn to create a count plot comparing the count of each movie rating\nsns.catplot(\n    data=df_filtered, y=\"rating\", hue=\"rating\", kind=\"count\",\n    palette=\"pastel\", edgecolor=\".6\",\n)\n\n\n\n\n\n\n\n\n\n\nPandas built-in plotting\nIn addition to libraries like Altair and Seaborn, Pandas has some built in charting functionality.\nWhile not as sophisticated as some of the other options, it is often good enough for quick visualizations.\nJust like with seaborn‚Äôs plotting functions, the pandas plotting functions return matplotlib axes objects, which can be further customized.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nemployees[ ['job','salary'] ].plot.box()\n\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = employees[ ['job','salary'] ].plot().box()\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\n\n# Use pandas' built in plotting functions to create a count plot comparing the count of each movie rating\n# This will be a little trickier than the other libraries, but one hint is that the pandas value_counts() function\n# actually returns a dataframe.\n\nimport matplotlib.pyplot as plt\n\nrating_counts = df_filtered[\"rating\"].value_counts()\n\nplt.figure(figsize=(9, 6))\n\nax = rating_counts.plot(kind=\"bar\")\n\nax.set_title(\"Rating Counts for Movies\")\nax.set_xlabel(\"Movie Rating\")\nax.set_ylabel(\"Number of Movies\")\n\nplt.show()"
  },
  {
    "objectID": "notebooks/Exploration_01.html#above-and-beyond",
    "href": "notebooks/Exploration_01.html#above-and-beyond",
    "title": "Data Exploration 01",
    "section": "üåü Above and Beyond üåü",
    "text": "üåü Above and Beyond üåü\nAfter reviewing your findings, the watchdog group would like some additional questions answered:\n\nHow are things affected if you include the ‚Äúmade for TV movies‚Äù that have been assigned TV ratings in your analysis, but still exclude unrated movies?\nThey would also like to see a separate report that includes only TV shows.\nFor an upcoming community meeting, the group would like to present a simple chart showing ‚ÄúFor Kids‚Äù and ‚ÄúFor Adults‚Äù categories. The easiest way to accomplish this would be to create a new column in your data frame that maps each rating to the appropriate ‚ÄúFor Kids‚Äù or ‚ÄúFor Adults‚Äù label, then create a new visualization based on that column.\n\n\nall_ratings = [\"NC-17\", \"TV-Y7-FV\", \"G\", \"TV-Y\", \"TV-Y7\", \"TV-G\", \"PG\", \"PG-13\", \"TV-PG\", \"R\", \"TV-14\", \"TV-MA\"]\n\ndf_ratedmovies = df_movies[df_movies[\"rating\"].isin(all_ratings)]\ndf_ratedmovies.head()\ndf_ratedmovies[\"rating\"].value_counts()\n\nallrating_counts = df_ratedmovies[\"rating\"].value_counts()\nplt.figure(figsize = (9,6))\nax = allrating_counts.plot(kind = \"bar\")\nax.set_title(\"Rating Counts For All Movies\")\nax.set_ylabel(\"Movie Count\")\nax.set_xlabel(\"Rating\")\n\nText(0.5, 0, 'Rating')\n\n\n\n\n\n\n\n\n\n\nEven with movies that are rated for TV included, it is clear that Netlix has much more content for Adults than they do for kids."
  },
  {
    "objectID": "notebooks/hint_xgboost.html",
    "href": "notebooks/hint_xgboost.html",
    "title": "Jacob Walker - Data Science Portfolio",
    "section": "",
    "text": "!pip install scikit-learn==1.5.2\n\n\nCollecting scikit-learn==1.5.2\n\n  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n\nRequirement already satisfied: numpy&gt;=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (2.0.2)\n\nRequirement already satisfied: scipy&gt;=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (1.16.3)\n\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (1.5.3)\n\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n\nDownloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 12.9/12.9 MB 49.9 MB/s eta 0:00:00\n\nInstalling collected packages: scikit-learn\n\n  Attempting uninstall: scikit-learn\n\n    Found existing installation: scikit-learn 1.6.1\n\n    Uninstalling scikit-learn-1.6.1:\n\n      Successfully uninstalled scikit-learn-1.6.1\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\numap-learn 0.5.11 requires scikit-learn&gt;=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n\nhdbscan 0.8.41 requires scikit-learn&gt;=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n\nSuccessfully installed scikit-learn-1.5.2\n\n\n\n\n\n# Load some test data\nimport pandas as pd\nmpg = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/mpg.csv')\nmpg.head()\n\n\n    \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n# Import the libraries we need\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\n\n\n# Horsepower has some missing values, represented as '?'\nmpg[mpg['horsepower'] == '?']\n\n\n    \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n32\n25.0\n4\n98.0\n?\n2046\n19.0\n71\n1\nford pinto\n\n\n126\n21.0\n6\n200.0\n?\n2875\n17.0\n74\n1\nford maverick\n\n\n330\n40.9\n4\n85.0\n?\n1835\n17.3\n80\n2\nrenault lecar deluxe\n\n\n336\n23.6\n4\n140.0\n?\n2905\n14.3\n80\n1\nford mustang cobra\n\n\n354\n34.5\n4\n100.0\n?\n2320\n15.8\n81\n2\nrenault 18i\n\n\n374\n23.0\n4\n151.0\n?\n3035\n20.5\n82\n1\namc concord dl\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n# Get our target variable and features and split them into test and train datasets\n\nX = mpg[['cylinders', 'displacement', 'acceleration', 'weight']]\ny = mpg['mpg']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Create the model and train it, use default hyperparameters for now\nmodel = XGBRegressor(\n    n_estimators=149,\n    learning_rate=0.05,\n    n_jobs=4\n)\nmodel.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=149,\n             n_jobs=4, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†XGBRegressor?Documentation for XGBRegressoriFittedXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.05, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=149,\n             n_jobs=4, num_parallel_tree=None, ...) \n\n\n\n# Get predictions for our test data\npredictions = model.predict(X_test)\npredictions\n\narray([38.093006 , 27.461372 , 22.79962  , 16.645338 , 15.378193 ,\n       29.41955  , 23.398054 , 12.267829 , 16.908375 , 19.940163 ,\n       14.9492445, 31.863993 , 35.887993 , 16.5434   , 27.60999  ,\n       13.194569 , 24.996582 , 19.156929 , 14.103415 , 37.346256 ,\n       32.364914 , 19.11032  , 19.063623 , 30.751842 , 15.607439 ,\n       28.053463 , 28.560078 , 26.008118 , 18.795809 , 11.531976 ,\n       23.269327 , 30.558887 , 16.128803 , 25.198778 , 35.487507 ,\n       14.383799 , 22.517305 , 20.64062  , 13.096999 , 29.229704 ,\n       22.646235 , 26.493925 , 22.085453 , 13.090571 , 28.941313 ,\n       26.843548 , 25.783709 , 26.329193 , 28.039436 , 28.239431 ,\n       20.80405  , 29.425743 , 30.69106  , 13.278478 , 25.092815 ,\n       13.74018  , 15.858232 , 24.208864 , 26.002754 , 20.382626 ,\n       14.559072 , 38.102917 , 26.578058 , 27.201317 , 19.473768 ,\n       24.155085 , 25.13554  , 30.973312 , 27.066053 , 15.259853 ,\n       32.575447 , 13.869861 , 13.796803 , 26.134579 , 20.052567 ,\n       21.71818  , 17.549486 , 28.110527 , 29.306341 , 16.3149   ],\n      dtype=float32)\n\n\n\n# Compute the Root Mean Squared Error of the predictions\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.metrics import r2_score\n\nresult = root_mean_squared_error(y_test, predictions)\nresult\n\n# Looks like we're off by just 4.5 mpg on average\n# Not bad for a quick run with no real preprocessing\n\nnp.float64(4.019845477561109)\n\n\n\nr2 = r2_score(y_test, predictions)\nr2\n\n0.6994564539435715\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/Exploration_04.html",
    "href": "notebooks/Exploration_04.html",
    "title": "Data Exploration 04",
    "section": "",
    "text": "You‚Äôre working with a team of botanists to develop a flower classification system.\nYour assignment is to build a k-Nearest Neighbors model to classify flowers based on their petal and sepal sizes."
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-a-import-and-explore-the-data",
    "href": "notebooks/Exploration_04.html#part-a-import-and-explore-the-data",
    "title": "Data Exploration 04",
    "section": "Part A: Import and Explore the data",
    "text": "Part A: Import and Explore the data\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/iris.csv\n\nInitial Data Analysis\nOnce you‚Äôve loaded the data, it‚Äôs a good idea to poke around a little bit to find out what you‚Äôre dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/iris.csv\")\ndf\n\n\n    \n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nIris-virginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nIris-virginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nIris-virginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nIris-virginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nIris-virginica\n\n\n\n\n150 rows √ó 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndf.describe()\n\n\n    \n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.054000\n3.758667\n1.198667\n\n\nstd\n0.828066\n0.433594\n1.764420\n0.763161\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB"
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-b-visualize-the-data",
    "href": "notebooks/Exploration_04.html#part-b-visualize-the-data",
    "title": "Data Exploration 04",
    "section": "Part B: Visualize the Data",
    "text": "Part B: Visualize the Data\nUse your preferred visualization library to create a scatterplot showing petal length vs petal width. You should plot each flower species as a different color on the scatter plot.\n\nfrom plotnine import *\n\n(\n\n ggplot(df, aes(x = \"petal_length\", y = \"petal_width\", color = \"species\"))+\n    geom_point()+\n    theme_bw()\n)"
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-c-prepare-the-data-for-machine-learning",
    "href": "notebooks/Exploration_04.html#part-c-prepare-the-data-for-machine-learning",
    "title": "Data Exploration 04",
    "section": "Part C: Prepare the Data for Machine Learning",
    "text": "Part C: Prepare the Data for Machine Learning\nData preparation (sometimes called ‚Äúdata wrangling‚Äù or ‚Äúdata munging‚Äù) is where you‚Äôll usually spend the bulk of your time when working on machine learning problems. Only rarely is data already in the optimal form for a given algorithm.\nOften we have to deal with missing values, normalize the data, and perform both simple and complex feature engineering to get the data into the form we need.\nOnce the data is in the correct form, we can then randomize the data and split it into training and test datasets (and sometimes an additional validation dataset).\n\nMachine Learning Steps\nAlmost universally, regardless of which algorithm or type of task we‚Äôre performing, building and evaluating a machine learning model with sklearn follows these steps:\n\nPerform any data preprocessing needed.\nPartition the data into features and targets.\nSplit the data into training and test sets (and sometimes a third validation set).\nCreate a configure whichever sklearn model object we‚Äôre using.\nTrain the model using its ‚Äúfit‚Äù method.\nTest the model using its ‚Äúpredict‚Äù method.\nUse a model evaluation metric to see how well the model performs.\n\nIf the model isn‚Äôt performing well, we will repeat one or more of the above steps (sometimes all of them).\nOnce the model is performing adequately, we‚Äôll deploy it for use as part of some larger system.\nFor now, let‚Äôs assume that this dataset is in the form we need, and we‚Äôll skip to step 2, partitioning the data.\n\n\nStep 2. Partition the Data into Features and Targets\nFirst, we‚Äôll create a dataframe called ‚ÄúX‚Äù containing the features of the data we want to use to make our predictions. In this case, that will be the sepal_length, sepal_width, petal_length, and petal_width features.\n(The name ‚ÄúX‚Äù isn‚Äôt special, but uppercase X is the conventional name for our feature dataset, because that‚Äôs what statisticians use to refer to a matrix of independent variables)\n\n# Create a new dataframe called X that contians the features we're going\n# to use to make predictions\n\nX = df[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n\nNext we‚Äôll create a dataframe called ‚Äúy‚Äù containing the target variable, or the set of values we want to predict. In this case, that will be species.\n(Once again, the name ‚Äúy‚Äù isn‚Äôt special, but lowercase y is the conventional name for a list of targets, because that‚Äôs what statisticians use to refer to a vector of dependent variables)\n\n# Create a new dataframe called y that contians the target we're\n# trying to predict\n\ny = df[\"species\"]\n\n\n\nStep 3. Split the data into training and test sets.\nNow that we have our data divided into features (X) and target values (y), we‚Äôll split each of these into a training set and a test set.\nWe‚Äôll use the training sets to ‚Äútrain‚Äù our model how to make predictions.\nWe‚Äôll then use our test sets to test how well our model has learned from the training data.\nWhile we could use a bunch of python code to do this step, the sklearn library has lots of built-in functions to handle common data manipulations related to machine learning.\nFor this step, we‚Äôll use the train_test_split() function.\n\n# Import and use the train_test_split() function to split the X and y\n# dataframes into training and test sets.\n#\n# The training data should contain 80% of the samples and\n# the test data should contain 20% of the samples.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42)\n\nAfter creating the training and test splits, output the head() of each one and notice how they row numbers have been randomized.\nAlso notice that X_train and y_train‚Äôs row numbers match up, as do X_test and y_test‚Äôs row numbers.\n\nX_train.head()\n\n\n    \n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n22\n4.6\n3.6\n1.0\n0.2\n\n\n15\n5.7\n4.4\n1.5\n0.4\n\n\n65\n6.7\n3.1\n4.4\n1.4\n\n\n11\n4.8\n3.4\n1.6\n0.2\n\n\n42\n4.4\n3.2\n1.3\n0.2\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\ny_train.head()\n\n\n\n\n\n\n\n\nspecies\n\n\n\n\n22\nIris-setosa\n\n\n15\nIris-setosa\n\n\n65\nIris-versicolor\n\n\n11\nIris-setosa\n\n\n42\nIris-setosa\n\n\n\n\ndtype: object"
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-d-create-and-train-a-model",
    "href": "notebooks/Exploration_04.html#part-d-create-and-train-a-model",
    "title": "Data Exploration 04",
    "section": "Part D: Create and Train a Model",
    "text": "Part D: Create and Train a Model\nWe‚Äôre going to create a model based on the k-Nearest Neighbors algorithm.\nSince this is a classification task, (we‚Äôre trying to classify which species a given flower belongs to), we‚Äôll use sklearn‚Äôs KNeighborsClassifer.\n\nStep 4. Create and configure the model\nWe start by importing the information about the model we want to create. In python, this information is called a class.\nThe KNeighborsClassifier class contains all of the information python needs to create a kNN Classifier.\nOnce we‚Äôve imported the class, we‚Äôll create an instance of the class using this syntax:\nwhatever = ClassName( parameter_one = value, parameter_two = something_else, etc...)\nIn our case, the class name is KNeighborsClassifer. It doesn‚Äôt matter what we call the variable that holds the instance, but one popular convention is to call classifier instances clf, so that‚Äôs what you‚Äôll see in the sklearn documentation.\nThe only parameter we want to configure is the n_neighbors parameter, which controls the value of k in the kNN algorithm.\n\n# Import the KNeighborsClassifier class from sklearn\n# Note that it's in the neighbors submodule. See the example code in the\n# documentation for details on how to import it\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# Create an instance of the model, configuring it to use the 3 nearest neighbors\n# store the instance in a variable\n\nneigh = KNeighborsClassifier(n_neighbors=3)\n\n\n\nStep 5: Train the model\nNext we‚Äôll train the model. We do this by providing it with the training data we split off from the dataset in step 3.\nThe model ‚Äúlearns‚Äù how to associate the feature values (X) with the targets (y). The exact process it uses to learn how to do this depends on which algorithm we‚Äôre using.\nSometimes, this is called ‚Äúfitting the data to the model‚Äù, so in sklearn, we perform this step using the fit() method.\n\n# Call the \"fit\" method of the classifier instance we created in step 4.\n# Pass it the X_train and y_train data so that it can learn to make predictions\n\nmymodel = neigh.fit(X_train, y_train)"
  },
  {
    "objectID": "notebooks/Exploration_04.html#part-e-make-predictions-and-evaluate-the-model",
    "href": "notebooks/Exploration_04.html#part-e-make-predictions-and-evaluate-the-model",
    "title": "Data Exploration 04",
    "section": "Part E: Make Predictions and Evaluate the Model",
    "text": "Part E: Make Predictions and Evaluate the Model\nNow that the model has been created and trained, we can use it to make predictions. Since this is a classification model, when we give it a set of features, it tells us what the most likely target value is.\nIn this case, we tell the model ‚Äúhere are the values for petal width, petal length, sepal width, and sepal length for a particular flower‚Äù The model then tells us which species is the most likely for that flower.\nWhen testing how well our model works, we‚Äôll use the test data we split off earlier. It contains the measurements for several flowers, along with their species.\n\nStep 6: Make Predictions on Test Data\nWe‚Äôll give the measurements of each flower to the model and have it predict their species. We‚Äôll then compare those predictions to the known values to determine how accurate our model is.\nSince this is a classification model, there are two different methods we can use to make predictions:\n\npredict(), which returns the most likely prediction for each sample.\npredict_proba() which returns a list of probabilities for each sample. The probabilities tell us how confident the model is that the corresponding sample belongs to a particular class.\n\n\n# Use the predict() method to get a list of predictions for the samples in our\n# test data. Then output those predictions\ny_pred = neigh.predict(X_test)\n\ny_pred\n\narray(['Iris-versicolor', 'Iris-setosa', 'Iris-virginica',\n       'Iris-versicolor', 'Iris-versicolor', 'Iris-setosa',\n       'Iris-versicolor', 'Iris-virginica', 'Iris-versicolor',\n       'Iris-versicolor', 'Iris-virginica', 'Iris-setosa', 'Iris-setosa',\n       'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica',\n       'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica',\n       'Iris-setosa', 'Iris-virginica', 'Iris-setosa', 'Iris-virginica',\n       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n       'Iris-virginica', 'Iris-setosa', 'Iris-setosa'], dtype=object)\n\n\n\n# Just a quick comparison with y_test to see if they match up\ny_test == y_pred\n\n\n\n\n\n\n\n\nspecies\n\n\n\n\n73\nTrue\n\n\n18\nTrue\n\n\n118\nTrue\n\n\n78\nTrue\n\n\n76\nTrue\n\n\n31\nTrue\n\n\n64\nTrue\n\n\n141\nTrue\n\n\n68\nTrue\n\n\n82\nTrue\n\n\n110\nTrue\n\n\n12\nTrue\n\n\n36\nTrue\n\n\n9\nTrue\n\n\n19\nTrue\n\n\n56\nTrue\n\n\n104\nTrue\n\n\n69\nTrue\n\n\n55\nTrue\n\n\n132\nTrue\n\n\n29\nTrue\n\n\n127\nTrue\n\n\n26\nTrue\n\n\n128\nTrue\n\n\n131\nTrue\n\n\n145\nTrue\n\n\n108\nTrue\n\n\n143\nTrue\n\n\n45\nTrue\n\n\n30\nTrue\n\n\n\n\ndtype: bool\n\n\n\n\nStep 7: Evaluate the Model\nThere are several metrics we can use to determine how well our model is performing.\nMost of them are in the sklearn.metrics library.\nMost of the sklearn metric function work using the same pattern. We import the function, then give it a list of the true values for our test data, and a list of the values the model predicted for our test data. The metric then outputs the value. How we interpret that value will depend on the exact problem we‚Äôre solving, the qualities of our data, and the particular metric we‚Äôre using.\n\nAccuracy\nSince this is a multiclass classification problem (‚Äúmulticlass‚Äù means we have more than two options we‚Äôre choosing from), we can get a quick estimate from the accuracy_score() function, which tells us the percent of correct predictions made by the model.\n\n# Import the accuracy_score function and use it to determine\n# how accurate the models predictions were for our test data\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n\n1.0\n\n\n\n\nConfusion Matrix\nWhile the accuracy score tells us a little about the model‚Äôs performance, it doesn‚Äôt tell us much.\nFor example, we know how often the model was correct, but we don‚Äôt know when it was wrong or why.\nWe can get this information from the confusion_matrix function.\n\n# Import the confusion_matrix function and use it to generate a confusion\n# matrix of our model results.\n\nfrom sklearn.metrics import confusion_matrix\nmymatrix = confusion_matrix(y_test, y_pred)\nmymatrix\n\n\n\nConfusion Matrix Plot\nIt‚Äôs easier to see the results of the confusion matrix if we plot the results. One way to do this is with Seaborn‚Äôs heatmap function.\nThis function works a little bit differently than the others. It takes as parameters your model instance, and then options for making the chart display the way you want, and outputs a confusion matrix showing how well the model did in predicting the target values.\nYou‚Äôll notice that in many cases (including this one), the numbers in the confusion matrix will be the same as the results you see from the confusion_matrix() function above, but the plot makes it easier to interpret the results.\nWhen using the confusion matrix, you may find that the default color mapping is difficult to read. The ‚ÄúBlues‚Äù mapping is a popular choice.\n\n# Create a Seaborn heatmap\nimport seaborn as sns\n\nsns.heatmap(mymatrix, annot=True, fmt=\".1f\")"
  },
  {
    "objectID": "notebooks/Exploration_04.html#above-and-beyond",
    "href": "notebooks/Exploration_04.html#above-and-beyond",
    "title": "Data Exploration 04",
    "section": "üåü Above and Beyond üåü",
    "text": "üåü Above and Beyond üåü\nOnce you‚Äôve complted the basics, try to complete one or more of the following tasks:\n\nSee if you can get better results from your model through some data preprocessing, such as normalization.\nOften, using too many features can give poor results. Can you get better performance using a subset of the features instead of all four?\nAre there other ways you could visualize your model results?"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 5"
    ]
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 4"
    ]
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 3"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 1"
    ]
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Jacob Walker‚Äôs CV",
    "section": "",
    "text": "Student And Data Operations\n\nLinkedIn\n\n\n\nBrigham Young University ‚Äì Idaho ‚Äì Rexburg, ID, Jan.¬†2024 ‚Äì Present\nBachelor of Science in Financial Economics | GPA 3.93\nExpected Graduation Date: July 2027\nChemeketa Community College ‚Äì Salem, OR, Sept.¬†2019 ‚Äì April 2021\n\n\n\n\nBank of Commerce: Data Operator/Fraud Specialist - January 2024‚ÄìPresent\n\nMaximized efficiency of the end-of-day bank operations by creating a list of all end-of-day operations, saving the bank more than 30 minutes each night.\nCollaborated with management to provide organizational oversight and guidance by creating a one-page guide and checklist, increasing proficiency in fraud detection between coworkers.\nAnalyzed the data of over 500 checks per day while ensuring key bank processes are completed at the indicated times.\n\nSolitude Landscaping: Crew Lead/Translator - March 2023‚Äì January 2024\n\nSupported management by increasing communication and understanding between crew leads by acting as a direct translator for the tie in the Spanish language.\nManaged and trained 10+ personnel.\nImplemented the use of data collectors to maximize the efficiency of crew routes and track time between stops.\nImproved weekly service efficiency by creating pruning and cutting schedules, increasing efficiency by 10%.\n\n\n\n\n\nMissionary for the Church of Jesus Christ of Latter-Day Saints ‚Äì January 2021 ‚Äì February 2023\n\nLearned and became fluent proficient in speaking and writing in the Spanish language within 3 months.\nOversaw 15+ missionary companionships and organized special trainings and councils.\nTrained two new missionaries in the Spanish language.\nPracticed budgeting personal finances with the allotted bi-monthly budget given.\n\nVolunteer Counselor for Amity Elementary School\nVarsity Captain for Amity High-School Cross Country Team\n\n\n\n\n\nCertified in the Spanish language through TrueNorth: Score 7.2\n\nProficient with Microsoft Office Suite\n\nProficient operating knowledge of the HORIZON banking system\nProficient in SQL\nProficient in R\nProficient in Python"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Jacob Walker‚Äôs CV",
    "section": "",
    "text": "Brigham Young University ‚Äì Idaho ‚Äì Rexburg, ID, Jan.¬†2024 ‚Äì Present\nBachelor of Science in Financial Economics | GPA 3.93\nExpected Graduation Date: July 2027\nChemeketa Community College ‚Äì Salem, OR, Sept.¬†2019 ‚Äì April 2021"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Jacob Walker‚Äôs CV",
    "section": "",
    "text": "Bank of Commerce: Data Operator/Fraud Specialist - January 2024‚ÄìPresent\n\nMaximized efficiency of the end-of-day bank operations by creating a list of all end-of-day operations, saving the bank more than 30 minutes each night.\nCollaborated with management to provide organizational oversight and guidance by creating a one-page guide and checklist, increasing proficiency in fraud detection between coworkers.\nAnalyzed the data of over 500 checks per day while ensuring key bank processes are completed at the indicated times.\n\nSolitude Landscaping: Crew Lead/Translator - March 2023‚Äì January 2024\n\nSupported management by increasing communication and understanding between crew leads by acting as a direct translator for the tie in the Spanish language.\nManaged and trained 10+ personnel.\nImplemented the use of data collectors to maximize the efficiency of crew routes and track time between stops.\nImproved weekly service efficiency by creating pruning and cutting schedules, increasing efficiency by 10%."
  },
  {
    "objectID": "resume.html#volunteer-service",
    "href": "resume.html#volunteer-service",
    "title": "Jacob Walker‚Äôs CV",
    "section": "",
    "text": "Missionary for the Church of Jesus Christ of Latter-Day Saints ‚Äì January 2021 ‚Äì February 2023\n\nLearned and became fluent proficient in speaking and writing in the Spanish language within 3 months.\nOversaw 15+ missionary companionships and organized special trainings and councils.\nTrained two new missionaries in the Spanish language.\nPracticed budgeting personal finances with the allotted bi-monthly budget given.\n\nVolunteer Counselor for Amity Elementary School\nVarsity Captain for Amity High-School Cross Country Team"
  },
  {
    "objectID": "resume.html#relevant-skills",
    "href": "resume.html#relevant-skills",
    "title": "Jacob Walker‚Äôs CV",
    "section": "",
    "text": "Certified in the Spanish language through TrueNorth: Score 7.2\n\nProficient with Microsoft Office Suite\n\nProficient operating knowledge of the HORIZON banking system\nProficient in SQL\nProficient in R\nProficient in Python"
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 2"
    ]
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/starter_housing.html",
    "href": "notebooks/starter_housing.html",
    "title": "Jacob Walker - Data Science Portfolio",
    "section": "",
    "text": "!pip install scikit-learn==1.5.2\n\n\nimport pandas as pd\n\nhousing = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')\n\nhousing\n\n\n    \n\n\n\n\n\n\nid\ndate\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\n...\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nprice\n\n\n\n\n0\n1565930130\n20141104T000000\n4\n3.25\n3760\n4675\n2.0\n0\n0\n3\n...\n2740\n1020\n2007\n0\n98038\n47.3862\n-122.048\n3280\n4033\n429900.0\n\n\n1\n3279000420\n20150115T000000\n3\n1.75\n1460\n7800\n1.0\n0\n0\n2\n...\n1040\n420\n1979\n0\n98023\n47.3035\n-122.382\n1310\n7865\n233000.0\n\n\n2\n194000575\n20141014T000000\n4\n1.00\n1340\n5800\n1.5\n0\n2\n3\n...\n1340\n0\n1914\n0\n98116\n47.5658\n-122.389\n1900\n5800\n455000.0\n\n\n3\n2115510160\n20141208T000000\n3\n1.75\n1440\n8050\n1.0\n0\n0\n3\n...\n1440\n0\n1985\n0\n98023\n47.3187\n-122.390\n1790\n7488\n258950.0\n\n\n4\n7522500005\n20140815T000000\n2\n1.50\n1780\n4750\n1.0\n0\n0\n4\n...\n1080\n700\n1947\n0\n98117\n47.6859\n-122.395\n1690\n5962\n555000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19995\n5272200045\n20141113T000000\n3\n1.50\n1000\n6914\n1.0\n0\n0\n3\n...\n1000\n0\n1947\n0\n98125\n47.7144\n-122.319\n1000\n6947\n378000.0\n\n\n19996\n9578500790\n20141111T000000\n3\n2.50\n3087\n5002\n2.0\n0\n0\n3\n...\n3087\n0\n2014\n0\n98023\n47.2974\n-122.349\n2927\n5183\n399950.0\n\n\n19997\n7202350480\n20140930T000000\n3\n2.50\n2120\n4780\n2.0\n0\n0\n3\n...\n2120\n0\n2004\n0\n98053\n47.6810\n-122.032\n1690\n2650\n575000.0\n\n\n19998\n1723049033\n20140620T000000\n1\n0.75\n380\n15000\n1.0\n0\n0\n3\n...\n380\n0\n1963\n0\n98168\n47.4810\n-122.323\n1170\n15000\n245000.0\n\n\n19999\n6147650280\n20150325T000000\n4\n2.50\n3130\n5999\n2.0\n0\n0\n3\n...\n3130\n0\n2006\n0\n98042\n47.3837\n-122.099\n3020\n5997\n315000.0\n\n\n\n\n20000 rows √ó 21 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nhousing['yr_built'].max()\n\n2015\n\n\n\nhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20000 entries, 0 to 19999\nData columns (total 21 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   id             20000 non-null  int64  \n 1   date           20000 non-null  object \n 2   bedrooms       20000 non-null  int64  \n 3   bathrooms      20000 non-null  float64\n 4   sqft_living    20000 non-null  int64  \n 5   sqft_lot       20000 non-null  int64  \n 6   floors         20000 non-null  float64\n 7   waterfront     20000 non-null  int64  \n 8   view           20000 non-null  int64  \n 9   condition      20000 non-null  int64  \n 10  grade          20000 non-null  int64  \n 11  sqft_above     20000 non-null  int64  \n 12  sqft_basement  20000 non-null  int64  \n 13  yr_built       20000 non-null  int64  \n 14  yr_renovated   20000 non-null  int64  \n 15  zipcode        20000 non-null  int64  \n 16  lat            20000 non-null  float64\n 17  long           20000 non-null  float64\n 18  sqft_living15  20000 non-null  int64  \n 19  sqft_lot15     20000 non-null  int64  \n 20  price          20000 non-null  float64\ndtypes: float64(5), int64(15), object(1)\nmemory usage: 3.2+ MB\n\n\n\nhousing.describe()\n\n\n    \n\n\n\n\n\n\nid\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\nprice\n\n\n\n\ncount\n2.000000e+04\n20000.000000\n20000.000000\n20000.000000\n2.000000e+04\n20000.00000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.000000\n20000.00000\n2.000000e+04\n\n\nmean\n4.580297e+09\n3.370200\n2.115750\n2079.883100\n1.501533e+04\n1.49645\n0.007550\n0.234900\n3.409700\n7.657700\n1790.232900\n289.650200\n1971.100150\n82.732150\n98077.876850\n47.559970\n-122.213897\n1986.387500\n12748.96040\n5.394367e+05\n\n\nstd\n2.875295e+09\n0.931931\n0.770134\n919.433672\n3.970135e+04\n0.54095\n0.086564\n0.767692\n0.651895\n1.175287\n829.587101\n441.537072\n29.374008\n397.872344\n53.438026\n0.138517\n0.140649\n685.211515\n27076.48904\n3.664334e+05\n\n\nmin\n1.000102e+06\n0.000000\n0.000000\n290.000000\n5.200000e+02\n1.00000\n0.000000\n0.000000\n1.000000\n1.000000\n290.000000\n0.000000\n1900.000000\n0.000000\n98001.000000\n47.155900\n-122.519000\n399.000000\n651.00000\n7.500000e+04\n\n\n25%\n2.123049e+09\n3.000000\n1.750000\n1430.000000\n5.040000e+03\n1.00000\n0.000000\n0.000000\n3.000000\n7.000000\n1200.000000\n0.000000\n1951.750000\n0.000000\n98033.000000\n47.470675\n-122.328000\n1490.000000\n5100.00000\n3.220000e+05\n\n\n50%\n3.904955e+09\n3.000000\n2.250000\n1910.000000\n7.620000e+03\n1.50000\n0.000000\n0.000000\n3.000000\n7.000000\n1560.000000\n0.000000\n1975.000000\n0.000000\n98065.000000\n47.571450\n-122.230000\n1840.000000\n7622.00000\n4.500000e+05\n\n\n75%\n7.309825e+09\n4.000000\n2.500000\n2550.000000\n1.071000e+04\n2.00000\n0.000000\n0.000000\n4.000000\n8.000000\n2210.000000\n560.000000\n1997.000000\n0.000000\n98117.000000\n47.678000\n-122.124000\n2360.000000\n10088.25000\n6.416250e+05\n\n\nmax\n9.900000e+09\n33.000000\n8.000000\n13540.000000\n1.164794e+06\n3.50000\n1.000000\n4.000000\n5.000000\n13.000000\n9410.000000\n4820.000000\n2015.000000\n2015.000000\n98199.000000\n47.777600\n-121.315000\n6210.000000\n871200.00000\n7.700000e+06\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10, 6))\nsns.histplot(housing['price'], bins=50, kde=True)\nplt.title('Distribution of Housing Prices')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom plotnine import *\n\n\n(\n    ggplot(housing, aes(y='price'))+\n    geom_boxplot(fill = \"steelblue\", color = \"black\")\n)\n\n\n\n\n\n\n\n\n\nfor column in housing.columns:\n    if column not in ['id', 'date', 'price']:\n        # Convert to category for better boxplot visualization if the column has few unique values\n        if housing[column].nunique() &lt; 50 and housing[column].dtype != 'object':\n            housing[column] = housing[column].astype('category')\n\n        print(f\"Generating boxplot for '{column}' vs 'price'...\")\n        p = (\n            ggplot(housing, aes(x=column, y='price')) +\n            geom_boxplot(fill=\"steelblue\", color=\"black\") +\n            labs(title=f'Price Distribution by {column}', x=column, y='Price') +\n            theme(axis_text_x=element_text(rotation=45, hjust=1))\n        )\n        print(p)\n\n# Special handling for 'date' if needed, though often not ideal for direct boxplot x-axis\n# If you wish to analyze date, you might extract year, month, or day of week first.\n\nGenerating boxplot for 'bedrooms' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'bathrooms' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'sqft_living' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'sqft_lot' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'floors' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'waterfront' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'view' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'condition' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'grade' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'sqft_above' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'sqft_basement' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'yr_built' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'yr_renovated' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'zipcode' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'lat' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'long' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'sqft_living15' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\nGenerating boxplot for 'sqft_lot15' vs 'price'...\n&lt;ggplot: (640 x 480)&gt;\n\n\n\nimport numpy as np\n\n# Select numeric columns from the one-hot encoded DataFrame\nnumeric_cols_encoded = housing_encoded.select_dtypes(include=[np.number])\n# Calculate correlation matrix\ncorr_encoded = numeric_cols_encoded.corr()\n\nplt.figure(figsize=(18, 15)) # Adjust figure size for more columns\nsns.heatmap(corr_encoded[['price']].sort_values('price', ascending=False),\n            annot=True, cmap='coolwarm', fmt=\".2f\", cbar=False)\nplt.title('Correlation of Numeric and One-Hot Encoded Features with Price')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Identify categorical columns to one-hot encode\n# Excluding 'id' and 'date' as they require different handling\ncategorical_cols = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'zipcode']\n\n# Ensure these columns are of 'category' dtype before one-hot encoding, if not already\nfor col in categorical_cols:\n    if col in housing.columns:\n        housing[col] = housing[col].astype('category')\n\n# Perform one-hot encoding\nhousing_encoded = pd.get_dummies(housing, columns=categorical_cols, drop_first=True)\n\n# Display the first few rows and the shape of the new DataFrame to show the effect of encoding\nprint(\"Shape of DataFrame after one-hot encoding:\", housing_encoded.shape)\ndisplay(housing_encoded.head())\n\nShape of DataFrame after one-hot encoding: (20000, 148)\n\n\n\n    \n\n\n\n\n\n\nid\ndate\nsqft_living\nsqft_lot\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nlat\nlong\n...\nzipcode_98146\nzipcode_98148\nzipcode_98155\nzipcode_98166\nzipcode_98168\nzipcode_98177\nzipcode_98178\nzipcode_98188\nzipcode_98198\nzipcode_98199\n\n\n\n\n0\n1565930130\n20141104T000000\n3760\n4675\n2740\n1020\n2007\n0\n47.3862\n-122.048\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n3279000420\n20150115T000000\n1460\n7800\n1040\n420\n1979\n0\n47.3035\n-122.382\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n194000575\n20141014T000000\n1340\n5800\n1340\n0\n1914\n0\n47.5658\n-122.389\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2115510160\n20141208T000000\n1440\n8050\n1440\n0\n1985\n0\n47.3187\n-122.390\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n7522500005\n20140815T000000\n1780\n4750\n1080\n700\n1947\n0\n47.6859\n-122.395\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows √ó 148 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\ntop_feats = ['price', 'sqft_living', 'sqft_lot', 'bedrooms', 'bathrooms', 'grade', 'sqft_living15']\nsns.pairplot(housing[top_feats], diag_kind='kde')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(data=housing, x='sqft_living', y='price', alpha=0.3)\nsns.regplot(data=housing, x='sqft_living', y='price',\n            scatter=False, color='red')\nplt.title('Price vs Sqft Living')\nplt.show()\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(data=housing, x='sqft_lot', y='price', alpha=0.3)\nplt.yscale('log')\nplt.xscale('log')\nplt.title('Price vs Lot Size (log-log)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor col in ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'grade', 'condition']:\n    plt.figure(figsize=(8,5))\n    sns.boxplot(data=housing, x=col, y='price')\n    plt.title(f'Price by {col}')\n    plt.xticks(rotation=45)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!pip install contextily\nimport contextily as cx\n\nplt.figure(figsize=(8,6))\nax = sns.scatterplot(data=housing, x='long', y='lat',\n                hue='price', palette='viridis', alpha=0.5)\nplt.title('Location of Homes Colored by Price')\n\n# Add a basemap to the plot\ncx.add_basemap(ax, crs='EPSG:4326') # Assuming longitude and latitude are in EPSG:4326\n\nplt.show()\n\nCollecting contextily\n  Downloading contextily-1.7.0-py3-none-any.whl.metadata (3.1 kB)\nRequirement already satisfied: geopy in /usr/local/lib/python3.12/dist-packages (from contextily) (2.4.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from contextily) (3.10.0)\nCollecting mercantile (from contextily)\n  Downloading mercantile-1.2.1-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from contextily) (11.3.0)\nRequirement already satisfied: rasterio in /usr/local/lib/python3.12/dist-packages (from contextily) (1.5.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from contextily) (2.32.4)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from contextily) (1.5.3)\nRequirement already satisfied: xyzservices in /usr/local/lib/python3.12/dist-packages (from contextily) (2025.11.0)\nRequirement already satisfied: geographiclib&lt;3,&gt;=1.52 in /usr/local/lib/python3.12/dist-packages (from geopy-&gt;contextily) (2.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;contextily) (1.3.3)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;contextily) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;contextily) (4.61.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;contextily) (1.4.9)\nRequirement already satisfied: numpy&gt;=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;contextily) (2.0.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;contextily) (26.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;contextily) (3.3.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib-&gt;contextily) (2.9.0.post0)\nRequirement already satisfied: click&gt;=3.0 in /usr/local/lib/python3.12/dist-packages (from mercantile-&gt;contextily) (8.3.1)\nRequirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio-&gt;contextily) (2.4.0)\nRequirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio-&gt;contextily) (25.4.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio-&gt;contextily) (2026.1.4)\nRequirement already satisfied: cligj&gt;=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio-&gt;contextily) (0.7.2)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;contextily) (3.4.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;contextily) (3.11)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;contextily) (2.5.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;contextily) (1.17.0)\nDownloading contextily-1.7.0-py3-none-any.whl (16 kB)\nDownloading mercantile-1.2.1-py3-none-any.whl (14 kB)\nInstalling collected packages: mercantile, contextily\nSuccessfully installed contextily-1.7.0 mercantile-1.2.1\n\n\n\n\n\n\n\n\n\nSuggested data cleaning actions:\nConvert date to a proper datetime and consider extracting year_sold or month_sold as separate features. ‚Äã\nCheck for extreme outliers in price, sqft_living, sqft_lot, and bedrooms (e.g., homes with 0 bedrooms or very large square footage) and decide whether to cap, transform, or remove them. ‚Äã\nEnsure categorical variables such as zipcode, view, waterfront, grade, condition, and floors are treated as categories (not continuous integers) before modeling. ‚Äã\nInvestigate yr_renovated (many zeros) and possibly create a binary feature like ‚Äúrenovated‚Äù plus ‚Äúage_since_renovation‚Äù instead of using the raw year. ‚Äã\nCheck for multicollinearity between highly correlated variables such as sqft_living, sqft_above, and sqft_living15, and consider dropping redundant features for simpler models. ‚Äã\nStandardize or scale numeric features (especially those with very different ranges like sqft_lot vs.¬†bathrooms) if you plan to use distance-based or regularized models. ‚Äã\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/module01-assessment.html#submission",
    "href": "notebooks/module01-assessment.html#submission",
    "title": "Introduction",
    "section": "Submission",
    "text": "Submission\nEach problem group asks a series of questions. This assignment consists of two submissions:\n\nAfter completing the questions below, open the Module 01 Assessment Quiz in Canvas and enter your answers to these questions there.\nAfter completing and submitting the quiz, save this Colab notebook as a GitHub Gist (You‚Äôll need to create a GitHub account for this), by selecting Save a copy as a GitHub Gist from the File menu above.\nIn Canvas, open the Module 01 Assessment GitHub Gist assignment and paste the GitHub Gist URL for this notebook. Then submit that assignment."
  },
  {
    "objectID": "notebooks/module01-assessment.html#problem-group-1",
    "href": "notebooks/module01-assessment.html#problem-group-1",
    "title": "Introduction",
    "section": "Problem Group 1",
    "text": "Problem Group 1\nFor the questions in this group, you‚Äôll work with the Netflix Movies Dataset found at this url: https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\n\nQuestion 1\nLoad the dataset into a Pandas data frame and determine what data type is used to store the release_year feature.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\")\ndf.head()\n\n\n    \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n0\n81145628\nMovie\nNorm of the North: King Sized Adventure\nRichard Finn, Tim Maltby\nAlan Marriott, Andrew Toth, Brian Dobson, Cole...\nUnited States, India, South Korea, China\nSeptember 9, 2019\n2019\nTV-PG\n90 min\nChildren & Family Movies, Comedies\nBefore planning an awesome wedding for his gra...\n\n\n1\n80117401\nMovie\nJandino: Whatever it Takes\nNaN\nJandino Asporaat\nUnited Kingdom\nSeptember 9, 2016\n2016\nTV-MA\n94 min\nStand-Up Comedy\nJandino Asporaat riffs on the challenges of ra...\n\n\n2\n70234439\nTV Show\nTransformers Prime\nNaN\nPeter Cullen, Sumalee Montano, Frank Welker, J...\nUnited States\nSeptember 8, 2018\n2013\nTV-Y7-FV\n1 Season\nKids' TV\nWith the help of three human allies, the Autob...\n\n\n3\n80058654\nTV Show\nTransformers: Robots in Disguise\nNaN\nWill Friedle, Darren Criss, Constance Zimmer, ...\nUnited States\nSeptember 8, 2018\n2016\nTV-Y7\n1 Season\nKids' TV\nWhen a prison ship crash unleashes hundreds of...\n\n\n4\n80125979\nMovie\n#realityhigh\nFernando Lebrija\nNesta Cooper, Kate Walsh, John Michael Higgins...\nUnited States\nSeptember 8, 2017\n2017\nTV-14\n99 min\nComedies\nWhen nerdy high schooler Dani finally attracts...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6234 entries, 0 to 6233\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   show_id       6234 non-null   int64 \n 1   type          6234 non-null   object\n 2   title         6234 non-null   object\n 3   director      4265 non-null   object\n 4   cast          5664 non-null   object\n 5   country       5758 non-null   object\n 6   date_added    6223 non-null   object\n 7   release_year  6234 non-null   int64 \n 8   rating        6224 non-null   object\n 9   duration      6234 non-null   object\n 10  listed_in     6234 non-null   object\n 11  description   6234 non-null   object\ndtypes: int64(2), object(10)\nmemory usage: 584.6+ KB\n\n\n\n\nQuestion 2\nFilter your dataset so it contains only TV Shows. How many of those TV Shows were rated TV-Y7?\n\ndf_tv = df[df[\"type\"] == \"TV Show\"]\n(df_tv[\"rating\"] == \"TV-Y7\").sum()\n\nnp.int64(100)\n\n\n\n\nQuestion 3\nFurther filter your dataset so it only contains TV Shows released between the years 2000 and 2009 inclusive. How many of those shows were rated TV-Y7?\n\ndf_2000s = df_tv[(df_tv[\"release_year\"] &gt;= 2000) & (df_tv[\"release_year\"] &lt;= 2009)]\n(df_2000s[\"rating\"] == \"TV-Y7\").sum()\n\nnp.int64(4)"
  },
  {
    "objectID": "notebooks/module01-assessment.html#problem-group-2",
    "href": "notebooks/module01-assessment.html#problem-group-2",
    "title": "Introduction",
    "section": "Problem Group 2",
    "text": "Problem Group 2\nFor the questions in this group, you‚Äôll work with the Cereal Dataset found at this url: https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\n\nQuestion 4\nAfter importing the dataset into a pandas data frame, determine the median amount of protein in cereal brands manufactured by Kelloggs. (mfr code ‚ÄúK‚Äù)\n\ndf_cereal = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\")\ndf_cereal\n\n\n    \n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n72\nTriples\nG\nC\n110\n2\n1\n250\n0.0\n21.0\n3\n60\n25\n3\n1.0\n0.75\n39.106174\n\n\n73\nTrix\nG\nC\n110\n1\n1\n140\n0.0\n13.0\n12\n25\n25\n2\n1.0\n1.00\n27.753301\n\n\n74\nWheat Chex\nR\nC\n100\n3\n1\n230\n3.0\n17.0\n3\n115\n25\n1\n1.0\n0.67\n49.787445\n\n\n75\nWheaties\nG\nC\n100\n3\n1\n200\n3.0\n17.0\n3\n110\n25\n1\n1.0\n1.00\n51.592193\n\n\n76\nWheaties Honey Gold\nG\nC\n110\n2\n1\n200\n1.0\n16.0\n8\n60\n25\n1\n1.0\n0.75\n36.187559\n\n\n\n\n77 rows √ó 16 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmedian_protein = round(df_cereal[\"protein\"].median(),2)\nmedian_protein\n\n3.0\n\n\n\n\nQuestion 5\nIn order to comply with new government regulations, all cereals must now come with a ‚ÄúHealthiness‚Äù rating. This rating is calculated based on this formula:\nhealthiness = (protein + fiber) / sugar\nCreate a new healthiness column populated with values based on the above formula.\nThen, determine the median healthiness value for only General Mills cereals (mfr = ‚ÄúG‚Äù), rounded to two decimal places.\n\ndf_cereal[\"healthiness\"] = ((df_cereal[\"protein\"] + df_cereal[\"fiber\"])/ df_cereal[\"sugars\"])\ndf_cereal.head()\n\n\n    \n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\nhealthiness\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n2.333333\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n0.625000\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n2.600000\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\ninf\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843\n0.375000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\nmedian_healthiness = round((df_cereal[df_cereal[\"mfr\"] == \"G\"][\"healthiness\"].median()),2)\nmedian_healthiness\n\n0.47"
  },
  {
    "objectID": "notebooks/module01-assessment.html#problem-group-3",
    "href": "notebooks/module01-assessment.html#problem-group-3",
    "title": "Introduction",
    "section": "Problem Group 3",
    "text": "Problem Group 3\nFor the questions in this group, you‚Äôll work with the Titanic Dataset found at this url: https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/titanic.csv\n\nQuestion 6\nAfter loading the dataset into a pandas DataFrame, create a new column called NameGroup that contains the first letter of the passenger‚Äôs surname in lower case.\nNote that in the dataset, passenger‚Äôs names are provided in the Name column and are listed as:\nSurname, Given names\nFor example, if a passenger‚Äôs Name is Braund, Mr. Owen Harris, the NameGroup column should contain the value b.\nThen count how many passengers have a NameGroup value of k.\n\ndf_titanic = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/titanic.csv\")\ndf_titanic[\"NameGroup\"] = df_titanic[\"Name\"].str.split(\",\").str[0].str[0].str.lower()\ndf_titanic\n\n\n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nNameGroup\n\n\n\n\n0\n1\nNo\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\nb\n\n\n1\n2\nYes\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\nc\n\n\n2\n3\nYes\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\nh\n\n\n3\n4\nYes\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\nf\n\n\n4\n5\nNo\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\na\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\nNo\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\nm\n\n\n887\n888\nYes\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\ng\n\n\n888\n889\nNo\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\nj\n\n\n889\n890\nYes\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\nb\n\n\n890\n891\nNo\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\nd\n\n\n\n\n891 rows √ó 13 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n(df_titanic[\"NameGroup\"] == \"k\").sum()\n\nnp.int64(28)"
  },
  {
    "objectID": "notebooks/Exploration_03.html",
    "href": "notebooks/Exploration_03.html",
    "title": "Data Exploration 03",
    "section": "",
    "text": "You‚Äôre working on an exhibit for a local museum called ‚ÄúThe Titanic Disaster‚Äù. They‚Äôve asked you to analyze the passenger manifests and see if you can find any interesting information for the exhibit.\nThe museum curator is particularly interested in why some people might have been more likely to survive than others."
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-1-import-pandas-and-load-the-data",
    "href": "notebooks/Exploration_03.html#part-1-import-pandas-and-load-the-data",
    "title": "Data Exploration 03",
    "section": "Part 1: Import Pandas and load the data",
    "text": "Part 1: Import Pandas and load the data\nRemember to import Pandas the conventional way. If you‚Äôve forgotten how, you may want to review Data Exploration 01.\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/titanic.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce you‚Äôve loaded the data, it‚Äôs a good idea to poke around a little bit to find out what you‚Äôre dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 1: Enter your code below to import Pandas according to the\n# conventional method. Then load the dataset into a Pandas dataframe.\nimport pandas as pd\n# Write any code needed to explore the data by seeing what the first few\n# rows look like. Then display a technical summary of the data to determine\n# the data types of each column, and which columns have missing data.\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/titanic.csv\")\ndf\n\n\n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\nNo\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\nYes\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\nYes\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\nYes\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\nNo\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\nNo\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\nYes\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\nNo\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\nYes\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\nNo\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows √ó 12 columns"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-2-initial-exploration",
    "href": "notebooks/Exploration_03.html#part-2-initial-exploration",
    "title": "Data Exploration 03",
    "section": "Part 2: Initial Exploration",
    "text": "Part 2: Initial Exploration\nUsing your visualization library of choice, let‚Äôs first look at some features in isolation. Generate visualizations showing:\n\nA comparison of the total number of passengers who survived compared to those that died.\nA comparison of the total number of males compared to females\nA histogram showing the distribution of sibling/spouse counts\nA histogram showing the distribution of parent/child counts\n\n\n# Part 2: # Write the code needed to generate the visualizations specified.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"darkgrid\")\n\n\nsns.histplot(data=df, x=\"Survived\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.histplot(data=df, x=\"Sex\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.histplot(data=df, x=\"SibSp\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.histplot(data=df, x=\"Parch\")\nplt.show()"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-3-pairwise-comparisons",
    "href": "notebooks/Exploration_03.html#part-3-pairwise-comparisons",
    "title": "Data Exploration 03",
    "section": "Part 3: Pairwise Comparisons",
    "text": "Part 3: Pairwise Comparisons\nUse your visualization library of choice to look at how the survival distribution varied across different groups.\n\nChoose some features that you think might have had some influence over the likelihood of a titanic passenger surviving.\nFor each of those features, generate a chart for each feature showing the survival distributions when taking that feature into account\n\n\n# Write the code to explore how different features affect the survival distribution\nfrom plotnine import *\n\n(\n    ggplot(df, aes(x=\"Pclass\", fill=\"factor(Survived)\"))\n    + geom_bar(position=\"dodge\")\n    + labs(\n        title=\"Survival distribution by Passenger Class\",\n        x=\"Passenger Class\",\n        y=\"Count\",\n        fill=\"Survived\"\n    )\n    + theme_minimal()\n)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(df, aes(x=\"Sex\", fill=\"Survived\"))+\n      geom_bar(position=\"dodge\")+\n      labs(\n        title=\"Survival distribution by Sex\",\n        x=\"Passenger Sex\",\n        y=\"Count\",\n        fill=\"Survived\")+\n      theme_minimal()\n)"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-4-feature-engineering",
    "href": "notebooks/Exploration_03.html#part-4-feature-engineering",
    "title": "Data Exploration 03",
    "section": "Part 4: Feature Engineering",
    "text": "Part 4: Feature Engineering\nThe museum curator wonders if the passenger‚Äôs rank and title might have anything to do with whether or not they survived. Since this information is embedded in their name, we‚Äôll use ‚Äúfeature engineering‚Äù to create two new columns:\n\nTitle: The passenger‚Äôs title\nRank: A boolean (true/false) indicating if a passenger was someone of rank.\n\nFor the first new column, you‚Äôll need to find a way to extract the title portion of their name. Be sure to clean up any whitespace or extra punctuation.\nFor the second new column, you‚Äôll need to first look at a summary of your list of titles and decide what exactly constitutes a title of rank. Will you include military and eccelsiastical titles? Once you‚Äôve made your decision, create the second column.\nYou may want to review prior Data Explorations for tips on creating new columns and checking for lists of values.\n\n# Finding Titles\n\ndf[\"Title\"] = (df[\"Name\"].str.extract(r\",\\s*([^.]*)\\.\", expand=False).str.strip()).str.title()\n\ndf[\"Title\"].unique()\n\narray(['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'The Countess',\n       'Jonkheer'], dtype=object)\n\n\n\n# Titles of Rank\n\nrank_titles = {\"Don\", \"Rev\", \"Dr\", \"Major\", \"Lady\", \"Sir\", \"Col\", \"Capt\", \"The Countess\", \"Jonkheer\"}\n\ndf[\"Rank\"] = df[\"Title\"].isin(rank_titles)\n\ndf\n\n\n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nTitle\nRank\n\n\n\n\n0\n1\nNo\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\nMr\nFalse\n\n\n1\n2\nYes\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\nMrs\nFalse\n\n\n2\n3\nYes\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\nMiss\nFalse\n\n\n3\n4\nYes\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\nMrs\nFalse\n\n\n4\n5\nNo\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\nMr\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\nNo\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\nRev\nTrue\n\n\n887\n888\nYes\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\nMiss\nFalse\n\n\n888\n889\nNo\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\nMiss\nFalse\n\n\n889\n890\nYes\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\nMr\nFalse\n\n\n890\n891\nNo\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\nMr\nFalse\n\n\n\n\n891 rows √ó 14 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nRevisit Visualizations\nNow that you have the new columns in place. Revisit the pairwise comparison plots to see if the new columns reveal any interesting relationships.\n\n# Enter the code needed to recheck the pairwise comparison.\n(ggplot(df, aes(x=\"Rank\", fill=\"Survived\"))+\n      geom_bar(position=\"dodge\")+\n      labs(\n        title=\"Survival distribution by Rank\",\n        x=\"Passenger Rank\",\n        y=\"Count\",\n        fill=\"Survived\")+\n      theme_minimal()\n)\n\n\n\n\n\n\n\n\n\n(ggplot(df, aes(x=\"Title\", fill=\"Survived\"))+\n      geom_bar(position=\"fill\")+\n      labs(title=\"Survival distribution by Title\",\n        x=\"Passenger Sex\",\n        y=\"Proportion\",\n        fill=\"Survived\")+\n      theme_minimal()+\n      theme(axis_text_x=element_text(rotation=45))\n)"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-5-encoding",
    "href": "notebooks/Exploration_03.html#part-5-encoding",
    "title": "Data Exploration 03",
    "section": "Part 5: Encoding",
    "text": "Part 5: Encoding\nThe museum has partnered with a data science group to build some interactive predicitive models using the titanic passenger data.\nMany machine learning algorithms require categorical features to be encoded as numbers.\nThere are two approaches to this, label encoding (sometimes called factorization), and ‚Äúone-hot‚Äù encoding.\n\nLabel Encoding\nLabel encoding creates numeric labels for each categorical value. For example, imagine we have a feature in the data called Pet with these values for the first five rows: ['Dog', 'Cat', 'Dog', 'Dog', 'Bird'].\nWe could create a new feature called Pet_Encoded where those values are represented as: [0, 1, 0, 0, 2]. Where 0 = Dog, 1 = Cat, and 2 = Bird.\nIn pandas there are two common ways to label encode a feature:\n\nMethod 1: factorize()\nFirst, we could pandas‚Äô factorize() method. It takes the series you want to encode as an argument and returns a list of two items.\nThe first item is an array of encoded values. The second is the set of original values.\n# The factorize() method returns the new values and the originals in a list.\n# So the [0] at the end indicates we want only the new values.\nmyData['Pet_Encoded'] = pd.factorize(myData['Pet'])[0]\n\n\nMethod 2: Category Data Type\nEvery column in a pandas dataframe is a certain datatype. Usually, pandas infers which datatype to use based on the values of the column. However, we can use the astype() method to convert a feature from one type to another.\nIf we first convert a feature to the category datatype, we can ask pandas to create a new column in the data frame based on the category codes:\n# Convert our column to the category type\nmyData['Pet'] = myData['Pet'].astype('category')\nmyData['Pet_Encoded'] = myData['Pet'].cat.codes\nWhichever method we choose, our machine learning algorithm could use the new Pet_Encoded feature in place of the Pet feature.\n\n# Create a new column in the dataset called \"Sex_Encoded\" containing the\n# label encoded values of the \"Sex\" column\n\ndf[\"Sex_Encoded\"] = pd.factorize(df['Sex'])[0]\ndf\n\n\n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nTitle\nRank\nSex_Encoded\n\n\n\n\n0\n1\nNo\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\nMr\nFalse\n0\n\n\n1\n2\nYes\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\nMrs\nFalse\n1\n\n\n2\n3\nYes\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\nMiss\nFalse\n1\n\n\n3\n4\nYes\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\nMrs\nFalse\n1\n\n\n4\n5\nNo\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\nMr\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\nNo\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\nRev\nTrue\n0\n\n\n887\n888\nYes\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\nMiss\nFalse\n1\n\n\n888\n889\nNo\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\nMiss\nFalse\n1\n\n\n889\n890\nYes\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\nMr\nFalse\n0\n\n\n890\n891\nNo\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\nMr\nFalse\n0\n\n\n\n\n891 rows √ó 15 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\nOne-Hot Encoding\nOne problem with label encoding is that it can make a categorical variable appear as if it contains a quantitative relationship between its values.\nIn the example above, is Bird twice as important as Cat? Some algorithms might interpret those values that way.\nOne-Hot encoding avoids this problem by creating a new feature for each category. The value of the new feature is either 0 (is not this value) or 1 (is this value).\nIn pandas, we can use the get_dummies() method to deal with this problem:\nmyEncodedData = pd.get_dummies(myData, columns=['Pet'])\nIn the case of our Pet example, the new features created by get_dummies() would be:\n\n\n\nPet_is_Dog\nPet_is_Cat\nPet_is_Bird\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n1\n0\n0\n\n\n0\n0\n1\n\n\n\nNotice that for our data, if Pet_is_Bird = 0 and Pet_is_Cat = 0, we know that the pet has to be a dog. So the Pet_is_Dog column contains redundant information. When this happens, we say that our data contains a multicollinearity problem.\nTo avoid this, we can tell get_dummies() that we want to get rid of one of the columns using the drop_first parameter:\nmyEncodedData = pd.get_dummies(myData, columns=['Pet'], drop_first=True)\nThe main disadvantage to One-Hot encoding is that if the feature you‚Äôre encoding has a lot of different values, it can result in a lot of extra features. This can sometimes lead to poor performance with some types of algorithms.\n\n# Use the pandas get_dummies() method to one-hot encode the Embarked column.\ndf_encoded = pd.get_dummies(df, columns=['Embarked'])\ndf_encoded\n\n\n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nTitle\nRank\nSex_Encoded\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\nNo\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nMr\nFalse\n0\nFalse\nFalse\nTrue\n\n\n1\n2\nYes\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nMrs\nFalse\n1\nTrue\nFalse\nFalse\n\n\n2\n3\nYes\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nMiss\nFalse\n1\nFalse\nFalse\nTrue\n\n\n3\n4\nYes\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nMrs\nFalse\n1\nFalse\nFalse\nTrue\n\n\n4\n5\nNo\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nMr\nFalse\n0\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\nNo\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nRev\nTrue\n0\nFalse\nFalse\nTrue\n\n\n887\n888\nYes\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nMiss\nFalse\n1\nFalse\nFalse\nTrue\n\n\n888\n889\nNo\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nMiss\nFalse\n1\nFalse\nFalse\nTrue\n\n\n889\n890\nYes\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nMr\nFalse\n0\nTrue\nFalse\nFalse\n\n\n890\n891\nNo\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nMr\nFalse\n0\nFalse\nTrue\nFalse\n\n\n\n\n891 rows √ó 17 columns"
  },
  {
    "objectID": "notebooks/Exploration_03.html#part-6-conclusions",
    "href": "notebooks/Exploration_03.html#part-6-conclusions",
    "title": "Data Exploration 03",
    "section": "Part 6: Conclusions",
    "text": "Part 6: Conclusions\nBased on your analysis, what interesting relationships did you find? Write three interesting facts the museum can use in their exhibit.\n\nOne interesting relationship I found was that there was nearly and equal proportion of people with ranks and without ranks that survived, showing that you Rank did not really matter, just that there were a larger amount of people present without ranks..\nIf you were a class three passenger, you were almost twice as likley to die compared to the other passenger classes.\nWomen had the greatest likleyhood of surviving the Titanic."
  },
  {
    "objectID": "notebooks/Exploration_03.html#above-and-beyond",
    "href": "notebooks/Exploration_03.html#above-and-beyond",
    "title": "Data Exploration 03",
    "section": "üåü Above and Beyond üåü",
    "text": "üåü Above and Beyond üåü\n\nThere appears to be a lot of different variations of similar titles. (such as abbreviations for Miss and Mademoiselle).\nScan through the different titles to see which titles can be consolidated, then use what you know about data manipulation to simplify the distribution.\nOnce you‚Äôve finished, check the visualizations again to see if that made any difference.\nThe museum curator has room for a couple of nice visualizations for the exhibit. Create additional visualizations that are suitable for public display.\n\n\n# Above and Beyond 1\n\ndf[\"Title\"] = df[\"Title\"].replace({\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Miss\",\n    \"Mme\": \"Mrs\"\n})\n\n\n(ggplot(df, aes(x=\"Title\", fill=\"Survived\"))+\n      geom_bar(position=\"dodge\")+\n      labs(title=\"Survival distribution by Title\",\n        x=\"Passenger Sex\",\n        y=\"Proportion\",\n        fill=\"Survived\")+\n      theme_minimal()+\n      theme(axis_text_x=element_text(rotation=45))\n)"
  },
  {
    "objectID": "notebooks/starter_bikes.html",
    "href": "notebooks/starter_bikes.html",
    "title": "Task",
    "section": "",
    "text": "import pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nbike = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n\nbike\n\n\n    \n\n\n\n\n\n\ndteday\nhr\ncasual\nregistered\ntemp_c\nfeels_like_c\nhum\nwindspeed\nweathersit\nseason\nholiday\nworkingday\n\n\n\n\n0\n1/1/2011\n0.0\n3\n13\n3.0\n3.0\n0.7957\n0.8\n1\n1\n0\n0\n\n\n1\n1/1/2011\n1.0\n8\n30\n1.7\n1.7\n0.8272\n0.8\n1\n1\n0\n0\n\n\n2\n1/1/2011\n2.0\n5\n26\n1.9\n1.9\n0.8157\n1.1\n1\n1\n0\n0\n\n\n3\n1/1/2011\n3.0\n3\n9\n2.5\n2.5\n0.7831\n0.8\n1\n1\n0\n0\n\n\n4\n1/1/2011\n4.0\n0\n1\n2.0\n2.0\n0.8075\n1.1\n1\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n112470\n10/31/2023\n19.0\n241\n594\n10.2\n10.2\n0.4516\n8.4\n2\n4\n0\n1\n\n\n112471\n10/31/2023\n20.0\n171\n450\n8.9\n7.5\n0.5268\n9.3\n1\n4\n0\n1\n\n\n112472\n10/31/2023\n21.0\n176\n402\n7.7\n6.4\n0.5756\n7.8\n1\n4\n0\n1\n\n\n112473\n10/31/2023\n22.0\n106\n257\n8.0\n8.0\n0.5604\n0.0\n1\n4\n0\n1\n\n\n112474\n10/31/2023\n23.0\n78\n135\n6.5\n6.5\n0.6401\n4.5\n1\n4\n0\n1\n\n\n\n\n112475 rows √ó 12 columns\nimport pandas as pd\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\n\n# -----------------------------\n# Load + parse dates\n# -----------------------------\nbike['dteday'] = pd.to_datetime(bike['dteday'])\n\n# Handy bounds (auto-adjust to your dataset)\nmin_day = bike['dteday'].min()\nmax_day = bike['dteday'].max()\n\n# -----------------------------\n# 1) COVID lockdown indicator (DC stay-at-home order period)\n#   DC stay-at-home order effective 2020-04-01; extended through mid/late May 2020.\n#   You can broaden this window if you mean \"COVID restriction era\" rather than strict stay-at-home.\n# -----------------------------\ncovid_lockdown_ranges = [\n    ('2020-04-01', '2020-05-29'),  # DC stay-at-home era (edit end date if your definition differs)\n]\n\nbike['covid_lockdown'] = 0\nfor start, end in covid_lockdown_ranges:\n    start = pd.to_datetime(start)\n    end = pd.to_datetime(end)\n    bike.loc[(bike['dteday'] &gt;= start) & (bike['dteday'] &lt;= end), 'covid_lockdown'] = 1\n\n# -----------------------------\n# 2) Government shutdown indicator (US federal shutdowns affecting DC)\n# -----------------------------\ngov_shutdown_ranges = [\n    ('2013-10-01', '2013-10-17'),  # 2013 shutdown\n    ('2018-01-20', '2018-01-22'),  # 2018 shutdown\n    ('2018-12-22', '2019-01-25'),  # 2018-2019 shutdown\n]\n\nbike['gov_shutdown'] = 0\nfor start, end in gov_shutdown_ranges:\n    start = pd.to_datetime(start)\n    end = pd.to_datetime(end)\n    bike.loc[(bike['dteday'] &gt;= start) & (bike['dteday'] &lt;= end), 'gov_shutdown'] = 1\n\n# -----------------------------\n# 3) Big event indicator (EDIT these to match what *you* consider \"big events\")\n#   Examples shown: major DC-centric events (inaugurations) + a couple of common big days.\n# -----------------------------\nbig_event_ranges = [\n    ('2013-01-18', '2013-01-22'),  # 2013 presidential inauguration weekend window\n    ('2017-01-19', '2017-01-21'),  # 2017 inauguration window\n    ('2021-01-19', '2021-01-21'),  # 2021 inauguration window\n    ('2015-09-22', '2015-09-24'),  # example: Papal visit window (edit if desired)\n    ('2020-06-01', '2020-06-02'),  # example: curfew/unrest peak window (edit if desired)\n]\n\nbike['big_event'] = 0\nfor start, end in big_event_ranges:\n    start = pd.to_datetime(start)\n    end = pd.to_datetime(end)\n    bike.loc[(bike['dteday'] &gt;= start) & (bike['dteday'] &lt;= end), 'big_event'] = 1\n\n# -----------------------------\n# 4) Major holiday indicator\n#   Strategy:\n#   - If your dataset has a \"holiday\" column (0/1), keep it as a baseline\n#   - Define \"major\" as: (holiday==1) AND also matches US federal holiday calendar\n#     OR (explicitly listed major fixed-date holidays like Jul 4 / Dec 25 / Jan 1).\n# -----------------------------\n# US federal holidays within data range\ncal = USFederalHolidayCalendar()\nfederal_holidays = cal.holidays(start=min_day, end=max_day)\n\n# Fixed-date major holidays (commonly treated as major regardless of calendar nuances)\nfixed_major_md = {(1, 1), (7, 4), (12, 25)}  # New Year, Independence Day, Christmas\nfixed_major = bike['dteday'].apply(lambda d: (d.month, d.day) in fixed_major_md)\n\n# If your dataset has 'holiday', incorporate it; otherwise just use calendar logic\nif 'holiday' in bike.columns:\n    bike['major_holiday'] = (\n        ((bike['holiday'] == 1) & (bike['dteday'].isin(federal_holidays) | fixed_major))\n        | fixed_major\n    ).astype(int)\nelse:\n    bike['major_holiday'] = (bike['dteday'].isin(federal_holidays) | fixed_major).astype(int)\n\n# -----------------------------\n# Quick sanity check\n# -----------------------------\ncols = ['dteday', 'covid_lockdown', 'gov_shutdown', 'big_event', 'major_holiday']\nprint(bike[cols].head(10))\nprint(\"\\nCounts:\\n\", bike[cols[1:]].sum())\nprint(bike)\n\n      dteday  covid_lockdown  gov_shutdown  big_event  major_holiday\n0 2011-01-01               0             0          0              1\n1 2011-01-01               0             0          0              1\n2 2011-01-01               0             0          0              1\n3 2011-01-01               0             0          0              1\n4 2011-01-01               0             0          0              1\n5 2011-01-01               0             0          0              1\n6 2011-01-01               0             0          0              1\n7 2011-01-01               0             0          0              1\n8 2011-01-01               0             0          0              1\n9 2011-01-01               0             0          0              1\n\nCounts:\n covid_lockdown    1416\ngov_shutdown      1320\nbig_event          384\nmajor_holiday     3384\ndtype: int64\n           dteday    hr  casual  registered  temp_c  feels_like_c     hum  \\\n0      2011-01-01   0.0       3          13     3.0           3.0  0.7957   \n1      2011-01-01   1.0       8          30     1.7           1.7  0.8272   \n2      2011-01-01   2.0       5          26     1.9           1.9  0.8157   \n3      2011-01-01   3.0       3           9     2.5           2.5  0.7831   \n4      2011-01-01   4.0       0           1     2.0           2.0  0.8075   \n...           ...   ...     ...         ...     ...           ...     ...   \n112470 2023-10-31  19.0     241         594    10.2          10.2  0.4516   \n112471 2023-10-31  20.0     171         450     8.9           7.5  0.5268   \n112472 2023-10-31  21.0     176         402     7.7           6.4  0.5756   \n112473 2023-10-31  22.0     106         257     8.0           8.0  0.5604   \n112474 2023-10-31  23.0      78         135     6.5           6.5  0.6401   \n\n        windspeed  weathersit  season  holiday  workingday  covid_lockdown  \\\n0             0.8           1       1        0           0               0   \n1             0.8           1       1        0           0               0   \n2             1.1           1       1        0           0               0   \n3             0.8           1       1        0           0               0   \n4             1.1           1       1        0           0               0   \n...           ...         ...     ...      ...         ...             ...   \n112470        8.4           2       4        0           1               0   \n112471        9.3           1       4        0           1               0   \n112472        7.8           1       4        0           1               0   \n112473        0.0           1       4        0           1               0   \n112474        4.5           1       4        0           1               0   \n\n        gov_shutdown  big_event  major_holiday  \n0                  0          0              1  \n1                  0          0              1  \n2                  0          0              1  \n3                  0          0              1  \n4                  0          0              1  \n...              ...        ...            ...  \n112470             0          0              0  \n112471             0          0              0  \n112472             0          0              0  \n112473             0          0              0  \n112474             0          0              0  \n\n[112475 rows x 16 columns]\nbike['dteday'] = pd.to_datetime(bike['dteday'])\nbike['date'] = bike['dteday'].dt.date\n\n# print(bike[['dteday', 'date']].head())\n\nbike['dteday'] = pd.to_datetime(bike['dteday'])\nbike['date'] = bike['dteday'].dt.date\n\n# print(bike[['dteday', 'date']].head())\n\nbike = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n\nbike['dteday'] = pd.to_datetime(bike['dteday'])\nbike['date'] = bike['dteday'].dt.date\n\n# print(bike[['dteday', 'date']].head())\n\ndaily_bike_data = bike.groupby('date').agg(\n    casual=('casual', 'sum'),\n    registered=('registered', 'sum'),\n    temp_c=('temp_c', 'mean'),\n    feels_like_c=('feels_like_c', 'mean'),\n    hum=('hum', 'mean'),\n    windspeed=('windspeed', 'mean'),\n    weathersit=('weathersit', 'first'),\n    season=('season', 'first'),\n    holiday=('holiday', 'first'),\n    workingday=('workingday', 'first')\n).reset_index()\n\ndaily_bike_data['day_of_week'] = pd.to_datetime(daily_bike_data['date']).dt.day_name()\n\nprint(daily_bike_data.head())\n\n         date  casual  registered    temp_c  feels_like_c       hum  \\\n0  2011-01-01     330         629  8.137500      7.970833  0.823567   \n1  2011-01-02     130         651  9.204167      7.429167  0.704825   \n2  2011-01-03     120        1181  0.558333     -3.729167  0.443213   \n3  2011-01-04     107        1429  0.550000     -2.350000  0.603488   \n4  2011-01-05      82        1489  2.237500     -0.891667  0.439533   \n\n   windspeed  weathersit  season  holiday  workingday day_of_week  \n0  10.754167           1       1        0           0    Saturday  \n1  16.783333           2       1        0           0      Sunday  \n2  16.750000           1       1        0           1      Monday  \n3  10.095833           1       1        0           1     Tuesday  \n4  12.700000           1       1        0           1   Wednesday\nbike.head()\n\n\n    \n\n\n\n\n\n\ndteday\nhr\ncasual\nregistered\ntemp_c\nfeels_like_c\nhum\nwindspeed\nweathersit\nseason\nholiday\nworkingday\ndate\n\n\n\n\n0\n2011-01-01\n0.0\n3\n13\n3.0\n3.0\n0.7957\n0.8\n1\n1\n0\n0\n2011-01-01\n\n\n1\n2011-01-01\n1.0\n8\n30\n1.7\n1.7\n0.8272\n0.8\n1\n1\n0\n0\n2011-01-01\n\n\n2\n2011-01-01\n2.0\n5\n26\n1.9\n1.9\n0.8157\n1.1\n1\n1\n0\n0\n2011-01-01\n\n\n3\n2011-01-01\n3.0\n3\n9\n2.5\n2.5\n0.7831\n0.8\n1\n1\n0\n0\n2011-01-01\n\n\n4\n2011-01-01\n4.0\n0\n1\n2.0\n2.0\n0.8075\n1.1\n1\n1\n0\n0\n2011-01-01\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Group by holiday and sum casual and registered bike usage\nholiday_bike_usage = daily_bike_data.groupby('holiday')[['casual', 'registered']].sum().reset_index()\n\n# Calculate total riders for each holiday category\nholiday_bike_usage['total'] = holiday_bike_usage['casual'] + holiday_bike_usage['registered']\n\n# Calculate proportions\nholiday_bike_usage['casual_proportion'] = holiday_bike_usage['casual'] / holiday_bike_usage['total']\nholiday_bike_usage['registered_proportion'] = holiday_bike_usage['registered'] / holiday_bike_usage['total']\n\n# Melt the DataFrame for easier plotting with seaborn, now with proportions\nholiday_bike_usage_melted = holiday_bike_usage.melt(id_vars='holiday', value_vars=['casual_proportion', 'registered_proportion'], var_name='user_type', value_name='proportion_of_riders')\n\n# Rename user_type for better legend labels\nholiday_bike_usage_melted['user_type'] = holiday_bike_usage_melted['user_type'].replace({\n    'casual_proportion': 'Casual Riders',\n    'registered_proportion': 'Registered Riders'\n})\n\n# Create the bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x='holiday', y='proportion_of_riders', hue='user_type', data=holiday_bike_usage_melted, palette='viridis')\n\n# Customize the plot\nplt.title('Proportion of Bike Usage: Regular Days vs. Holidays')\nplt.xlabel('Day Type (0 = Regular Day, 1 = Holiday)')\nplt.ylabel('Proportion of Riders')\nplt.xticks(ticks=[0, 1], labels=['Regular Day', 'Holiday'])\nplt.legend(title='User Type')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\nThis bar chart visually compares the total number of casual and registered bike riders on regular days versus holidays. You can see how the usage patterns differ between these two categories."
  },
  {
    "objectID": "notebooks/starter_bikes.html#calculate-total-riders",
    "href": "notebooks/starter_bikes.html#calculate-total-riders",
    "title": "Task",
    "section": "Calculate Total Riders",
    "text": "Calculate Total Riders\n\nSubtask:\nAdd a ‚Äòtotal_riders‚Äô column to the daily_bike_data DataFrame by summing the ‚Äòcasual‚Äô and ‚Äòregistered‚Äô columns. This will be used for the bar chart.\nReasoning: To complete the subtask, I will add a new column ‚Äòtotal_riders‚Äô to the daily_bike_data DataFrame by summing the ‚Äòcasual‚Äô and ‚Äòregistered‚Äô columns. Then, I will display the head of the DataFrame to verify the new column.\n\ndaily_bike_data['total_riders'] = daily_bike_data['casual'] + daily_bike_data['registered']\n\nprint(daily_bike_data.head())\n\n         date  casual  registered    temp_c  feels_like_c       hum  \\\n0  2011-01-01     330         629  8.137500      7.970833  0.823567   \n1  2011-01-02     130         651  9.204167      7.429167  0.704825   \n2  2011-01-03     120        1181  0.558333     -3.729167  0.443213   \n3  2011-01-04     107        1429  0.550000     -2.350000  0.603488   \n4  2011-01-05      82        1489  2.237500     -0.891667  0.439533   \n\n   windspeed  weathersit  season  holiday  workingday day_of_week  \\\n0  10.754167           1       1        0           0    Saturday   \n1  16.783333           2       1        0           0      Sunday   \n2  16.750000           1       1        0           1      Monday   \n3  10.095833           1       1        0           1     Tuesday   \n4  12.700000           1       1        0           1   Wednesday   \n\n   total_riders  \n0           959  \n1           781  \n2          1301  \n3          1536  \n4          1571"
  },
  {
    "objectID": "notebooks/starter_bikes.html#create-combined-bar-and-line-chart",
    "href": "notebooks/starter_bikes.html#create-combined-bar-and-line-chart",
    "title": "Task",
    "section": "Create Combined Bar and Line Chart",
    "text": "Create Combined Bar and Line Chart\n\nSubtask:\nGenerate a plot where bars represent the ‚Äòtotal_riders‚Äô from daily_bike_data for each day, and overlaid lines show the ‚Äòcasual‚Äô users (green) and ‚Äòregistered‚Äô users (blue) over the same period. Ensure proper labeling for axes and a title for the chart.\nReasoning: I need to generate a combined bar and line chart as described in the subtask. This involves creating a bar plot for total riders and overlaying line plots for casual and registered riders on a secondary y-axis. I will use matplotlib.pyplot and seaborn to achieve this, ensuring all labels, titles, and legends are correctly placed.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.dates as mdates\n\n# Ensure 'date' column is in datetime format for proper plotting\ndaily_bike_data['date'] = pd.to_datetime(daily_bike_data['date'])\n\n# Create a figure and a set of subplots\nfig, ax1 = plt.subplots(figsize=(15, 8))\n\n# Convert dates to numerical format so both bar and line plots share the same continuous date axis\ndates_numeric = mdates.date2num(daily_bike_data['date'])\n\n# Plot total_riders as a bar chart on ax1 using numerical dates\nbar_width = 0.8\nax1.bar(dates_numeric, daily_bike_data['total_riders'], width=bar_width, color='#cccccc', label='Total Riders', align='center')\n\n# Create a second y-axis that shares the same x-axis as ax1\nax2 = ax1.twinx()\n\n# Plot casual riders as a green line on ax2\nax2.plot(dates_numeric, daily_bike_data['casual'], color='green', label='Casual Riders', linewidth=2)\n\n# Plot registered riders as a blue line on ax2\nax2.plot(dates_numeric, daily_bike_data['registered'], color='blue', label='Registered Riders', linewidth=2)\n\n# Set the title of the chart\nplt.title('Daily Bike Usage: Total, Casual, and Registered Riders', fontsize=16)\n\n# Set x-axis label for ax1\nax1.set_xlabel('Date', fontsize=12)\n\n# Set y-axis label for ax1\nax1.set_ylabel('Total Riders', color='#cccccc', fontsize=12)\nax1.tick_params(axis='y', labelcolor='#cccccc')\n\n# Set y-axis label for ax2\nax2.set_ylabel('Number of Casual/Registered Riders', color='black', fontsize=12)\n\n# Format the x-axis to display dates properly\nax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\nax1.xaxis.set_major_locator(mdates.AutoDateLocator())\n\n# Combine legends from both axes\nhandles1, labels1 = ax1.get_legend_handles_labels()\nhandles2, labels2 = ax2.get_legend_handles_labels()\nax2.legend(handles1 + handles2, labels1 + labels2, loc='upper left', fontsize=10)\n\n# Rotate x-axis labels if they overlap to improve readability\nfig.autofmt_xdate()\n\n# Add grid for better readability\nax1.grid(True, linestyle='--', alpha=0.6)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nReasoning: The previous code failed because sns.barplot likely coerced the ‚Äòdate‚Äô column into a categorical type for the x-axis, which then caused a TypeError when ax2.plot tried to interpret the same ‚Äòdate‚Äô column as a datetime object for plotting lines. To fix this, I will use ax1.bar instead of sns.barplot to plot the total riders, using the numerical representation of dates for the x-axis. This ensures that the x-axis is treated as a continuous datetime scale, allowing the line plots on ax2 to render correctly without conflicts.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.dates as mdates\n\n# Ensure 'date' column is in datetime format for proper plotting\ndaily_bike_data['date'] = pd.to_datetime(daily_bike_data['date'])\n\n# Create a figure and a set of subplots\nfig, ax1 = plt.subplots(figsize=(15, 8))\n\n# Convert dates to numerical format for plotting with plt.bar and plt.plot\n# This ensures a continuous date axis consistent across both axes\ndates_numeric = mdates.date2num(daily_bike_data['date'])\n\n# Plot total_riders as a bar chart on ax1 using numerical dates\n# We need to set the width of the bars appropriately for daily data\nbar_width = 0.8 # Adjust as needed\nax1.bar(dates_numeric, daily_bike_data['total_riders'], width=bar_width, color='#cccccc', label='Total Riders', align='center')\n\n# Create a second y-axis that shares the same x-axis as ax1\nax2 = ax1.twinx()\n\n# Plot casual riders as a green line on ax2 using numerical dates\nax2.plot(dates_numeric, daily_bike_data['casual'], color='green', label='Casual Riders', linewidth=2)\n\n# Plot registered riders as a blue line on ax2 using numerical dates\nax2.plot(dates_numeric, daily_bike_data['registered'], color='blue', label='Registered Riders', linewidth=2)\n\n# Set the title of the chart\nplt.title('Daily Bike Usage: Total, Casual, and Registered Riders', fontsize=16)\n\n# Set x-axis label for ax1\nax1.set_xlabel('Date', fontsize=12)\n\n# Set y-axis label for ax1\nax1.set_ylabel('Total Riders', color='#555555', fontsize=12) # Adjusted color for visibility\nax1.tick_params(axis='y', labelcolor='#555555')\n\n# Set y-axis label for ax2\nax2.set_ylabel('Number of Casual/Registered Riders', color='black', fontsize=12)\n\n# Format the x-axis to display dates properly\nax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\nax1.xaxis.set_major_locator(mdates.AutoDateLocator())\n\n# Combine legends from both axes\nhandles1, labels1 = ax1.get_legend_handles_labels()\nhandles2, labels2 = ax2.get_legend_handles_labels()\nax2.legend(handles1 + handles2, labels1 + labels2, loc='upper left', fontsize=10)\n\n# Rotate x-axis labels if they overlap to improve readability\nfig.autofmt_xdate()\n\n# Add grid for better readability\nax1.grid(True, linestyle='--', alpha=0.6)\n\n# Display the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/starter_bikes.html#final-task",
    "href": "notebooks/starter_bikes.html#final-task",
    "title": "Task",
    "section": "Final Task",
    "text": "Final Task\n\nSubtask:\nSummarize the insights observed from the combined bar and line chart, highlighting trends in casual, registered, and total bike usage over time."
  },
  {
    "objectID": "notebooks/starter_bikes.html#summary",
    "href": "notebooks/starter_bikes.html#summary",
    "title": "Task",
    "section": "Summary:",
    "text": "Summary:\n\nQ&A\nThe combined bar and line chart reveals distinct trends in casual, registered, and total bike usage over time. Registered users consistently form the majority of riders and show a much higher usage volume compared to casual users. Both casual and registered riders exhibit a clear seasonal pattern, with usage generally lower in colder months (e.g., early 2011) and significantly higher during warmer periods. Total ridership, driven primarily by registered users, mirrors this seasonal trend.\n\n\nData Analysis Key Findings\n\nA total_riders column was successfully added to the daily_bike_data DataFrame by summing ‚Äòcasual‚Äô and ‚Äòregistered‚Äô users. For example, on 2011-01-01, there were 330 casual and 629 registered riders, totaling 959 riders.\nThe generated chart clearly illustrates that registered riders (blue line) significantly outnumber casual riders (green line) throughout the observed period.\nBoth casual and registered rider numbers, and consequently total_riders (grey bars), demonstrate a strong seasonal pattern, with usage peaking during warmer months and declining during colder periods.\nThe total_riders bar height closely follows the registered riders line, indicating that registered users are the primary drivers of overall daily bike usage.\n\n\n\nInsights or Next Steps\n\nTargeted Marketing Opportunities: Given the dominance of registered users, efforts could be focused on retaining and expanding this group through loyalty programs or incentives. For casual users, understanding what barriers prevent them from registering could unlock growth.\nSeasonal Operational Adjustments: The clear seasonal trend suggests that resource allocation (e.g., bike availability, maintenance, staffing) could be optimized to match demand fluctuations, with higher capacity planned for peak seasons and reduced operations during off-peak times."
  },
  {
    "objectID": "notebooks/starter_bikes.html#prepare-data-for-hourly-analysis",
    "href": "notebooks/starter_bikes.html#prepare-data-for-hourly-analysis",
    "title": "Task",
    "section": "Prepare Data for Hourly Analysis",
    "text": "Prepare Data for Hourly Analysis\n\nSubtask:\nConvert the ‚Äòdteday‚Äô column in the bike DataFrame to datetime objects, then extract the ‚Äòday_of_week‚Äô to enable hourly and daily analysis.\nReasoning: First, I‚Äôll convert the ‚Äòdteday‚Äô column to datetime objects to ensure consistency and then extract the ‚Äòday_of_week‚Äô to a new column as instructed. Finally, I will display the head of the DataFrame to verify the changes.\n\nbike['dteday'] = pd.to_datetime(bike['dteday'])\nbike['day_of_week'] = bike['dteday'].dt.day_name()\n\nprint(bike.head())\n\n      dteday   hr  casual  registered  temp_c  feels_like_c     hum  \\\n0 2011-01-01  0.0       3          13     3.0           3.0  0.7957   \n1 2011-01-01  1.0       8          30     1.7           1.7  0.8272   \n2 2011-01-01  2.0       5          26     1.9           1.9  0.8157   \n3 2011-01-01  3.0       3           9     2.5           2.5  0.7831   \n4 2011-01-01  4.0       0           1     2.0           2.0  0.8075   \n\n   windspeed  weathersit  season  holiday  workingday        date day_of_week  \n0        0.8           1       1        0           0  2011-01-01    Saturday  \n1        0.8           1       1        0           0  2011-01-01    Saturday  \n2        1.1           1       1        0           0  2011-01-01    Saturday  \n3        0.8           1       1        0           0  2011-01-01    Saturday  \n4        1.1           1       1        0           0  2011-01-01    Saturday"
  },
  {
    "objectID": "notebooks/starter_bikes.html#aggregate-hourly-bike-usage",
    "href": "notebooks/starter_bikes.html#aggregate-hourly-bike-usage",
    "title": "Task",
    "section": "Aggregate Hourly Bike Usage",
    "text": "Aggregate Hourly Bike Usage\n\nSubtask:\nGroup the bike DataFrame by ‚Äòday_of_week‚Äô and ‚Äòhr‚Äô, and then sum the ‚Äòcasual‚Äô and ‚Äòregistered‚Äô columns to get the total usage for each hour of each day of the week.\nReasoning: To complete the subtask, I will group the bike DataFrame by ‚Äòday_of_week‚Äô and ‚Äòhr‚Äô, calculate the sum of ‚Äòcasual‚Äô and ‚Äòregistered‚Äô riders, reset the index, and store the result in hourly_bike_usage.\n\nhourly_bike_usage = bike.groupby(['day_of_week', 'hr']).agg(\n    casual=('casual', 'sum'),\n    registered=('registered', 'sum')\n).reset_index()\n\nprint(hourly_bike_usage.head())\n\n  day_of_week   hr  casual  registered\n0      Friday  0.0   13673       34099\n1      Friday  1.0    7443       16459\n2      Friday  2.0    4240        8396\n3      Friday  3.0    1993        4040\n4      Friday  4.0    1891        6254"
  },
  {
    "objectID": "notebooks/starter_bikes.html#generate-hourly-bike-usage-line-charts",
    "href": "notebooks/starter_bikes.html#generate-hourly-bike-usage-line-charts",
    "title": "Task",
    "section": "Generate Hourly Bike Usage Line Charts",
    "text": "Generate Hourly Bike Usage Line Charts\n\nSubtask:\nCreate line charts showing casual and registered bike usage across hours, faceted by each day of the week, with different colored lines for casual and registered users. Ensure proper labeling for axes and a title for the chart.\nReasoning: To create line charts showing casual and registered bike usage faceted by day of the week, I need to first prepare the data by melting the hourly_bike_usage DataFrame into a long format suitable for seaborn.relplot. Then, I will generate the plot with the specified parameters.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define the order of days for consistent plotting\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n# Convert 'hr' to int type to remove '.0'\nhourly_bike_usage['hr'] = hourly_bike_usage['hr'].astype(int)\n\n# Melt the DataFrame to long format for plotting casual and registered as separate lines\nhourly_bike_usage_melted = hourly_bike_usage.melt(\n    id_vars=['day_of_week', 'hr'],\n    value_vars=['casual', 'registered'],\n    var_name='user_type',\n    value_name='total_riders'\n)\n\n# Create the relplot\ng = sns.relplot(\n    data=hourly_bike_usage_melted,\n    x='hr',\n    y='total_riders',\n    kind='line',\n    col='day_of_week',\n    col_order=day_order,\n    hue='user_type',\n    col_wrap=4,\n    height=3.5,\n    aspect=1.2,\n    palette={'casual': 'green', 'registered': 'blue'}\n)\n\n# Set descriptive titles and labels\ng.fig.suptitle('Hourly Bike Usage by Day of Week', y=1.02, fontsize=16)\ng.set_axis_labels('Hour of Day', 'Number of Riders')\n\n# Adjust x-axis ticks to show all hours clearly\nfor ax in g.axes.flat:\n    ax.set_xticks(range(0, 24, 2)) # Show every other hour\n    ax.tick_params(axis='x', rotation=45)\n    ax.grid(True, linestyle='--', alpha=0.6)\n\n# Improve layout\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/starter_bikes.html#summarize-hourly-bike-usage-trends",
    "href": "notebooks/starter_bikes.html#summarize-hourly-bike-usage-trends",
    "title": "Task",
    "section": "Summarize Hourly Bike Usage Trends",
    "text": "Summarize Hourly Bike Usage Trends\n\nSubtask:\nAnalyze and summarize the observed trends in casual and registered bike usage across different hours of the day and days of the week from the generated line charts."
  },
  {
    "objectID": "notebooks/starter_bikes.html#summary-1",
    "href": "notebooks/starter_bikes.html#summary-1",
    "title": "Task",
    "section": "Summary:",
    "text": "Summary:\n\nQ&A\nThe analysis revealed distinct hourly bike usage trends for both casual and registered users across different days of the week. Registered users predominantly exhibit commuting patterns with morning and evening peaks on weekdays, while casual users show leisure-oriented usage with a broader midday to afternoon peak, particularly strong on weekends.\n\n\nData Analysis Key Findings\n\nThe bike DataFrame was successfully prepared by converting the dteday column to datetime objects and extracting the day_of_week to facilitate hourly and daily usage analysis.\nHourly bike usage was aggregated by summing casual and registered riders for each hour of each day, creating a comprehensive hourly_bike_usage dataset. For example, on a Friday at hour 0, there were 13673 casual riders and 34099 registered riders.\nThe generated line charts revealed that registered users typically display a bimodal usage pattern on weekdays, with prominent peaks during morning (e.g., 7-9 AM) and evening (e.g., 5-7 PM) commuting hours. On weekends, their usage is generally lower and shows a single, broader peak, often in the afternoon.\nCasual users exhibit a more leisure-oriented unimodal pattern, with usage peaking broadly from midday to afternoon (e.g., 10 AM - 4 PM) on both weekdays and weekends. Their overall usage tends to be higher or more comparable to registered users during peak weekend leisure hours.\nOverall, bike usage for both casual and registered riders is significantly lower during early morning hours (e.g., 1 AM - 5 AM) across all days of the week.\n\n\n\nInsights or Next Steps\n\nThe observed distinct usage patterns between casual and registered riders and across weekdays/weekends can inform demand-side management strategies, such as dynamic pricing or promotional campaigns tailored to specific user types and times.\nFurther analysis could explore the impact of weather conditions, holidays, or special events on these hourly and daily usage trends to provide a more complete picture of demand drivers.\n\n\nbike\n\n\n    \n\n\n\n\n\n\ndteday\nhr\ncasual\nregistered\ntemp_c\nfeels_like_c\nhum\nwindspeed\nweathersit\nseason\nholiday\nworkingday\ndate\nday_of_week\n\n\n\n\n0\n2011-01-01\n0.0\n3\n13\n3.0\n3.0\n0.7957\n0.8\n1\n1\n0\n0\n2011-01-01\nSaturday\n\n\n1\n2011-01-01\n1.0\n8\n30\n1.7\n1.7\n0.8272\n0.8\n1\n1\n0\n0\n2011-01-01\nSaturday\n\n\n2\n2011-01-01\n2.0\n5\n26\n1.9\n1.9\n0.8157\n1.1\n1\n1\n0\n0\n2011-01-01\nSaturday\n\n\n3\n2011-01-01\n3.0\n3\n9\n2.5\n2.5\n0.7831\n0.8\n1\n1\n0\n0\n2011-01-01\nSaturday\n\n\n4\n2011-01-01\n4.0\n0\n1\n2.0\n2.0\n0.8075\n1.1\n1\n1\n0\n0\n2011-01-01\nSaturday\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n112470\n2023-10-31\n19.0\n241\n594\n10.2\n10.2\n0.4516\n8.4\n2\n4\n0\n1\n2023-10-31\nTuesday\n\n\n112471\n2023-10-31\n20.0\n171\n450\n8.9\n7.5\n0.5268\n9.3\n1\n4\n0\n1\n2023-10-31\nTuesday\n\n\n112472\n2023-10-31\n21.0\n176\n402\n7.7\n6.4\n0.5756\n7.8\n1\n4\n0\n1\n2023-10-31\nTuesday\n\n\n112473\n2023-10-31\n22.0\n106\n257\n8.0\n8.0\n0.5604\n0.0\n1\n4\n0\n1\n2023-10-31\nTuesday\n\n\n112474\n2023-10-31\n23.0\n78\n135\n6.5\n6.5\n0.6401\n4.5\n1\n4\n0\n1\n2023-10-31\nTuesday\n\n\n\n\n112475 rows √ó 14 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nfrom google.colab import files\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\n\n# ‚îÄ‚îÄ Re-add all engineered features ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nbike['dteday'] = pd.to_datetime(bike['dteday'])\nmin_day = bike['dteday'].min()\nmax_day = bike['dteday'].max()\n\n# 1) covid_lockdown\ncovid_lockdown_ranges = [('2020-04-01', '2021-07-25')]\nbike['covid_lockdown'] = 0\nfor start, end in covid_lockdown_ranges:\n    bike.loc[(bike['dteday'] &gt;= pd.to_datetime(start)) & (bike['dteday'] &lt;= pd.to_datetime(end)), 'covid_lockdown'] = 1\n\n# 2) gov_shutdown\ngov_shutdown_ranges = [('2013-10-01','2013-10-17'), ('2018-01-20','2018-01-22'), ('2018-12-22','2019-01-25')]\nbike['gov_shutdown'] = 0\nfor start, end in gov_shutdown_ranges:\n    bike.loc[(bike['dteday'] &gt;= pd.to_datetime(start)) & (bike['dteday'] &lt;= pd.to_datetime(end)), 'gov_shutdown'] = 1\n\n# 3) big_event\nbig_event_ranges = [\n    # Presidential Inaugurations\n    ('2013-01-18', '2013-01-22'), ('2017-01-19', '2017-01-21'), ('2021-01-19', '2021-01-21'),\n    # State of the Union / Joint Session Addresses\n    ('2011-01-25', '2011-01-25'), ('2012-01-24', '2012-01-24'), ('2013-02-12', '2013-02-12'),\n    ('2014-01-28', '2014-01-28'), ('2015-01-20', '2015-01-20'), ('2016-01-12', '2016-01-12'),\n    ('2017-02-28', '2017-02-28'), ('2018-01-30', '2018-01-30'), ('2019-02-05', '2019-02-05'),\n    ('2020-02-04', '2020-02-04'), ('2021-04-28', '2021-04-28'), ('2022-03-01', '2022-03-01'),\n    ('2023-02-07', '2023-02-07'),\n    # Major Marches & Protests\n    ('2017-01-21', '2017-01-21'), ('2018-01-20', '2018-01-20'), ('2019-01-19', '2019-01-19'), ('2020-01-18', '2020-01-18'),\n    ('2018-03-24', '2018-03-24'), ('2017-04-22', '2017-04-22'),\n    ('2011-01-22', '2011-01-22'), ('2012-01-23', '2012-01-23'), ('2013-01-25', '2013-01-25'),\n    ('2014-01-22', '2014-01-22'), ('2015-01-22', '2015-01-22'), ('2016-01-22', '2016-01-22'),\n    ('2017-01-27', '2017-01-27'), ('2018-01-19', '2018-01-19'), ('2019-01-18', '2019-01-18'),\n    ('2020-01-24', '2020-01-24'), ('2022-01-21', '2022-01-21'), ('2023-01-20', '2023-01-20'),\n    ('2020-05-29', '2020-06-07'), ('2021-01-06', '2021-01-06'),\n    # Other Major DC Events\n    ('2015-09-22', '2015-09-24'), ('2014-11-11', '2014-11-11'),\n    ('2017-04-29', '2017-04-29'), ('2018-08-12', '2018-08-12'),\n]\nbike['big_event'] = 0\nfor start, end in big_event_ranges:\n    bike.loc[(bike['dteday'] &gt;= pd.to_datetime(start)) & (bike['dteday'] &lt;= pd.to_datetime(end)), 'big_event'] = 1\n\n# 4) major_holiday\ncal = USFederalHolidayCalendar()\nfederal_holidays = cal.holidays(start=min_day, end=max_day)\nfixed_major_md = {(1,1),(7,4),(12,25)}\nfixed_major = bike['dteday'].apply(lambda d: (d.month, d.day) in fixed_major_md)\nbike['major_holiday'] = (((bike['holiday'] == 1) & (bike['dteday'].isin(federal_holidays) | fixed_major)) | fixed_major).astype(int)\n\n# 5) date and day_of_week\nbike['date'] = bike['dteday'].dt.date\nbike['day_of_week'] = bike['dteday'].dt.day_name()\n\n# 6) weather_emergency (NEW)\n# Blizzards, hurricanes, derechos, and other major weather events in DC (2011-2023)\nweather_emergency_ranges = [\n    # --- Major Blizzards / Snowstorms ---\n    # Jan 2011 snowstorm (~5\" at DCA, widespread disruption)\n    ('2011-01-26', '2011-01-27'),\n    # Feb 2014 snowstorm (~6\" + ice, 2nd major storm of brutal winter)\n    ('2014-02-12', '2014-02-13'),\n    # Mar 2014 snowstorm (continuation of brutal 2013-14 winter)\n    ('2014-03-17', '2014-03-17'),\n    # Jan 2016 Blizzard Jonas / Snowzilla (17.8\" at DCA - historic, state of emergency)\n    ('2016-01-22', '2016-01-24'),\n    # Jan 2019 major snowstorm (8.3\" at DCA)\n    ('2019-01-13', '2019-01-14'),\n    # Feb 2019 snowstorm\n    ('2019-02-11', '2019-02-12'),\n    # Jan 2022 snowstorm (6.9\" at DCA)\n    ('2022-01-03', '2022-01-04'),\n    # Jan 2016 also had a second smaller storm\n    ('2016-02-15', '2016-02-16'),\n    # Dec 2020 snowstorm\n    ('2020-12-16', '2020-12-17'),\n    # Feb 2015 back-to-back snowstorms (Juno aftermath + multiple storms)\n    ('2015-02-16', '2015-02-17'),\n    ('2015-03-05', '2015-03-06'),\n    # Feb 2013 Nemo (DC fringe, significant snow)\n    ('2013-02-08', '2013-02-09'),\n    # --- Hurricanes & Tropical Storms ---\n    # Hurricane Irene - heavy rain, flooding, wind damage in DC\n    ('2011-08-27', '2011-08-28'),\n    # Hurricane Sandy - historic storm, extensive DC-area damage, state of emergency\n    ('2012-10-28', '2012-10-30'),\n    # Tropical Storm Isaias - power outages, downed trees across DC metro\n    ('2020-08-04', '2020-08-04'),\n    # --- Derecho & Extreme Wind Events ---\n    # Derecho of June 29, 2012 - 70+ mph gusts, 1M+ without power, 22 deaths\n    ('2012-06-29', '2012-06-29'),\n    # --- Earthquake ---\n    # 5.8 magnitude Virginia earthquake felt strongly throughout DC\n    ('2011-08-23', '2011-08-23'),\n]\nbike['weather_emergency'] = 0\nfor start, end in weather_emergency_ranges:\n    bike.loc[(bike['dteday'] &gt;= pd.to_datetime(start)) & (bike['dteday'] &lt;= pd.to_datetime(end)), 'weather_emergency'] = 1\n\n# ‚îÄ‚îÄ Sanity check\nprint('weather_emergency flagged dates:')\nprint(bike[bike['weather_emergency'] == 1][['dteday']].drop_duplicates().sort_values('dteday').to_string())\nprint(f'\\nTotal weather_emergency rows: {bike[\"weather_emergency\"].sum()}')\nprint(f'Total columns: {bike.shape[1]}')\n\n# ‚îÄ‚îÄ Export\nbike.to_csv('bikes_with_features.csv', index=False)\nprint('\\nColumns included:', list(bike.columns))\nprint(f'Shape: {bike.shape}')\nprint('CSV saved as bikes_with_features.csv')\nfiles.download('bikes_with_features.csv')\n\nweather_emergency flagged dates:\n          dteday\n600   2011-01-26\n624   2011-01-27\n5615  2011-08-23\n5711  2011-08-27\n5735  2011-08-28\n13078 2012-06-29\n15982 2012-10-28\n16006 2012-10-29\n16030 2012-10-30\n18454 2013-02-08\n18478 2013-02-09\n27309 2014-02-12\n27333 2014-02-13\n28100 2014-03-17\n36164 2015-02-16\n36188 2015-02-17\n36572 2015-03-05\n36596 2015-03-06\n44323 2016-01-22\n44347 2016-01-23\n44371 2016-01-24\n44899 2016-02-15\n44923 2016-02-16\n70408 2019-01-13\n70432 2019-01-14\n71104 2019-02-11\n71128 2019-02-12\n84062 2020-08-04\n87278 2020-12-16\n87302 2020-12-17\n96469 2022-01-03\n96493 2022-01-04\n\nTotal weather_emergency rows: 768\nTotal columns: 19\n\nColumns included: ['dteday', 'hr', 'casual', 'registered', 'temp_c', 'feels_like_c', 'hum', 'windspeed', 'weathersit', 'season', 'holiday', 'workingday', 'date', 'day_of_week', 'covid_lockdown', 'gov_shutdown', 'big_event', 'major_holiday', 'weather_emergency']\nShape: (112475, 19)\nCSV saved as bikes_with_features.csv\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# ‚îÄ‚îÄ 1. Prepare features & target ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n# Target: total hourly ridership\nbike['total_riders'] = bike['casual'] + bike['registered']\n\n# Encode day_of_week as integer (Monday=0 ... Sunday=6)\nday_map = {'Monday':0,'Tuesday':1,'Wednesday':2,'Thursday':3,\n           'Friday':4,'Saturday':5,'Sunday':6}\nbike['day_of_week_num'] = bike['day_of_week'].map(day_map)\n\n# Feature columns\nfeature_cols = [\n    'hr',            # hour of day\n    'temp_c',        # temperature\n    'feels_like_c',  # feels-like temperature\n    'hum',           # humidity\n    'windspeed',     # wind speed\n    'weathersit',    # weather situation code\n    'season',        # season (1-4)\n    'holiday',       # public holiday flag\n    'workingday',    # working day flag\n    'day_of_week_num',  # day of week (0-6)\n    'covid_lockdown',   # COVID lockdown period\n    'gov_shutdown',     # government shutdown period\n    'big_event',        # major DC events\n    'major_holiday',    # major holiday\n    'weather_emergency' # blizzards, hurricanes, etc.\n]\n\nX = bike[feature_cols].values\ny = bike['total_riders'].values\n\n# ‚îÄ‚îÄ 2. Scale features ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ‚îÄ‚îÄ 3. Train / Validation / Test split (70 / 15 / 15) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X_scaled, y, test_size=0.30, random_state=42\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.50, random_state=42\n)\n\nprint(f'Train:      {X_train.shape[0]:,} rows')\nprint(f'Validation: {X_val.shape[0]:,} rows')\nprint(f'Test:       {X_test.shape[0]:,} rows')\n\n# ‚îÄ‚îÄ 4. Build the Neural Network ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ntf.random.set_seed(111)\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=len(X_train[0]), activation='relu'))\nmodel.add(Dropout(.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(64, activation='leaky_relu'))\nmodel.add(Dense(1, activation='relu'))\n\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\nmodel.summary()\n\n# ‚îÄ‚îÄ 5. Train ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    batch_size=256,\n    callbacks=[early_stop],\n    verbose=1\n)\n\n# ‚îÄ‚îÄ 6. Evaluate on test set ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ntest_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\nprint(f'\\nTest MSE: {test_loss:.2f}')\nprint(f'Test MAE: {test_mae:.2f} riders')\n\ny_pred = model.predict(X_test).flatten()\nss_res = np.sum((y_test - y_pred) ** 2)\nss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\nr2 = 1 - ss_res / ss_tot\nprint(f'Test R¬≤:  {r2:.4f}')\n\n# ‚îÄ‚îÄ 7. Plot training curves ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\naxes[0].plot(history.history['loss'], label='Train Loss')\naxes[0].plot(history.history['val_loss'], label='Val Loss')\naxes[0].set_title('Loss (MSE) Over Epochs')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('MSE')\naxes[0].legend()\naxes[0].grid(True, linestyle='--', alpha=0.6)\n\naxes[1].plot(history.history['mae'], label='Train MAE')\naxes[1].plot(history.history['val_mae'], label='Val MAE')\naxes[1].set_title('MAE Over Epochs')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('MAE (riders)')\naxes[1].legend()\naxes[1].grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n\n# ‚îÄ‚îÄ 8. Predicted vs Actual scatter (sample 2000 pts for clarity) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nsample_idx = np.random.choice(len(y_test), size=2000, replace=False)\nfig2, ax = plt.subplots(figsize=(7, 6))\nax.scatter(y_test[sample_idx], y_pred[sample_idx], alpha=0.3, s=10, color='steelblue')\nmax_val = max(y_test.max(), y_pred.max())\nax.plot([0, max_val], [0, max_val], 'r--', linewidth=1.5, label='Perfect fit')\nax.set_xlabel('Actual Riders')\nax.set_ylabel('Predicted Riders')\nax.set_title(f'Predicted vs Actual  (R¬≤ = {r2:.4f})')\nax.legend()\nax.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\nprint('\\nDone.')\n\nTrain:      78,732 rows\nValidation: 16,871 rows\nTest:       16,872 rows\n\n\n/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nModel: \"sequential_3\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense_12 (Dense)                ‚îÇ (None, 128)            ‚îÇ         2,048 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout_5 (Dropout)             ‚îÇ (None, 128)            ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_13 (Dense)                ‚îÇ (None, 256)            ‚îÇ        33,024 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_14 (Dense)                ‚îÇ (None, 64)             ‚îÇ        16,448 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_15 (Dense)                ‚îÇ (None, 1)              ‚îÇ            65 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 51,585 (201.50 KB)\n\n\n\n Trainable params: 51,585 (201.50 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nEpoch 1/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 10ms/step - loss: 152501.0000 - mae: 279.4078 - val_loss: 74134.0547 - val_mae: 200.1946\n\nEpoch 2/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 76752.6797 - mae: 202.9598 - val_loss: 65868.1094 - val_mae: 183.6300\n\nEpoch 3/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 68735.0859 - mae: 187.1489 - val_loss: 55513.5938 - val_mae: 161.4163\n\nEpoch 4/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 60470.6328 - mae: 169.7561 - val_loss: 49300.6875 - val_mae: 146.9074\n\nEpoch 5/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 54761.6719 - mae: 158.1217 - val_loss: 46864.2188 - val_mae: 139.5348\n\nEpoch 6/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 51921.9805 - mae: 152.6435 - val_loss: 45619.4570 - val_mae: 136.9275\n\nEpoch 7/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 49876.2031 - mae: 148.5215 - val_loss: 44276.8594 - val_mae: 135.8661\n\nEpoch 8/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 48813.0430 - mae: 146.4271 - val_loss: 43409.6797 - val_mae: 133.8442\n\nEpoch 9/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 47962.5430 - mae: 144.9780 - val_loss: 42720.8008 - val_mae: 131.9946\n\nEpoch 10/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 47062.9492 - mae: 143.1583 - val_loss: 42040.6875 - val_mae: 131.2746\n\nEpoch 11/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 46126.9414 - mae: 142.0711 - val_loss: 41193.9492 - val_mae: 130.1528\n\nEpoch 12/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 45297.8164 - mae: 140.1482 - val_loss: 40697.0273 - val_mae: 128.5506\n\nEpoch 13/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 44839.7578 - mae: 139.6460 - val_loss: 39594.1602 - val_mae: 127.0315\n\nEpoch 14/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 43893.0781 - mae: 137.7982 - val_loss: 38993.4023 - val_mae: 126.2840\n\nEpoch 15/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 43113.9570 - mae: 136.3700 - val_loss: 37659.6719 - val_mae: 124.9191\n\nEpoch 16/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 42126.8867 - mae: 135.0425 - val_loss: 36426.1094 - val_mae: 123.1340\n\nEpoch 17/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 41190.5234 - mae: 133.4366 - val_loss: 35378.1875 - val_mae: 120.9585\n\nEpoch 18/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 40426.3477 - mae: 132.0047 - val_loss: 34400.6445 - val_mae: 119.4459\n\nEpoch 19/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 38900.5469 - mae: 129.8069 - val_loss: 33594.4570 - val_mae: 117.9269\n\nEpoch 20/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 38091.7266 - mae: 128.4335 - val_loss: 32272.2734 - val_mae: 116.7060\n\nEpoch 21/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 37192.6367 - mae: 127.0074 - val_loss: 31694.8281 - val_mae: 114.5729\n\nEpoch 22/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 36024.1836 - mae: 124.9552 - val_loss: 31001.0723 - val_mae: 113.8579\n\nEpoch 23/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 35562.9492 - mae: 123.7791 - val_loss: 30556.0859 - val_mae: 112.0332\n\nEpoch 24/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 34622.7930 - mae: 122.3952 - val_loss: 29733.5742 - val_mae: 111.8574\n\nEpoch 25/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 33973.4492 - mae: 121.3241 - val_loss: 29041.8887 - val_mae: 110.1698\n\nEpoch 26/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 33173.3008 - mae: 120.0674 - val_loss: 28617.0723 - val_mae: 109.1703\n\nEpoch 27/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 32851.1602 - mae: 119.1616 - val_loss: 28347.8574 - val_mae: 108.6753\n\nEpoch 28/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 32402.0352 - mae: 118.3535 - val_loss: 28135.3164 - val_mae: 108.3932\n\nEpoch 29/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 32027.5664 - mae: 118.0120 - val_loss: 27585.8066 - val_mae: 107.3315\n\nEpoch 30/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 31633.8184 - mae: 116.8752 - val_loss: 27211.4238 - val_mae: 106.6708\n\nEpoch 31/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 31552.1172 - mae: 117.1889 - val_loss: 27351.0801 - val_mae: 106.5609\n\nEpoch 32/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 31296.9395 - mae: 116.8520 - val_loss: 27023.6055 - val_mae: 106.8063\n\nEpoch 33/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 31119.7441 - mae: 116.3007 - val_loss: 26480.6699 - val_mae: 104.8046\n\nEpoch 34/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 30371.5273 - mae: 114.7110 - val_loss: 26131.1680 - val_mae: 104.1957\n\nEpoch 35/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 30001.2480 - mae: 113.9055 - val_loss: 25863.2969 - val_mae: 103.4615\n\nEpoch 36/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 30052.2012 - mae: 113.9486 - val_loss: 25587.4609 - val_mae: 102.4599\n\nEpoch 37/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 29594.4043 - mae: 112.7697 - val_loss: 26128.6543 - val_mae: 103.1150\n\nEpoch 38/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 29526.3945 - mae: 112.5382 - val_loss: 25630.0352 - val_mae: 102.9069\n\nEpoch 39/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 29353.8750 - mae: 112.2332 - val_loss: 25748.7207 - val_mae: 102.8455\n\nEpoch 40/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 29032.9180 - mae: 111.8667 - val_loss: 25268.9590 - val_mae: 101.8018\n\nEpoch 41/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 28903.0449 - mae: 111.3795 - val_loss: 24787.0312 - val_mae: 100.8669\n\nEpoch 42/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 28581.3848 - mae: 111.1541 - val_loss: 24492.7871 - val_mae: 100.1240\n\nEpoch 43/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 28529.4629 - mae: 111.0053 - val_loss: 24853.5605 - val_mae: 101.1915\n\nEpoch 44/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 28445.9746 - mae: 110.7087 - val_loss: 25024.2656 - val_mae: 100.9988\n\nEpoch 45/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 27906.2344 - mae: 109.6537 - val_loss: 24660.1367 - val_mae: 100.6702\n\nEpoch 46/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 28115.4980 - mae: 110.2361 - val_loss: 23887.3262 - val_mae: 98.8253\n\nEpoch 47/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 27615.6660 - mae: 109.1585 - val_loss: 23930.5762 - val_mae: 98.9298\n\nEpoch 48/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 4ms/step - loss: 27626.0312 - mae: 109.3007 - val_loss: 23718.6035 - val_mae: 98.4578\n\nEpoch 49/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 27757.0273 - mae: 109.6322 - val_loss: 25175.4434 - val_mae: 101.7248\n\nEpoch 50/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 27657.7891 - mae: 109.5590 - val_loss: 23271.6602 - val_mae: 97.6312\n\nEpoch 51/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 27766.7383 - mae: 109.6592 - val_loss: 23557.3008 - val_mae: 98.4395\n\nEpoch 52/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 27180.8672 - mae: 108.4292 - val_loss: 23757.2363 - val_mae: 98.2678\n\nEpoch 53/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26990.9414 - mae: 108.1149 - val_loss: 23045.5391 - val_mae: 97.5090\n\nEpoch 54/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 27195.6992 - mae: 108.9309 - val_loss: 23271.6582 - val_mae: 97.6311\n\nEpoch 55/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 26794.2656 - mae: 108.0009 - val_loss: 22969.2539 - val_mae: 96.8344\n\nEpoch 56/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 26720.8535 - mae: 107.5512 - val_loss: 23264.3613 - val_mae: 97.9195\n\nEpoch 57/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 27101.1270 - mae: 108.6479 - val_loss: 22809.9863 - val_mae: 97.0108\n\nEpoch 58/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26658.2109 - mae: 107.8101 - val_loss: 23095.3145 - val_mae: 97.5613\n\nEpoch 59/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26703.6973 - mae: 107.8433 - val_loss: 22885.4277 - val_mae: 96.7381\n\nEpoch 60/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 26565.3418 - mae: 107.5362 - val_loss: 22557.8008 - val_mae: 96.5564\n\nEpoch 61/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 26472.9043 - mae: 107.4292 - val_loss: 22885.5605 - val_mae: 97.1474\n\nEpoch 62/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26105.7754 - mae: 106.6863 - val_loss: 23000.4473 - val_mae: 97.7772\n\nEpoch 63/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26280.6777 - mae: 107.4029 - val_loss: 22639.9199 - val_mae: 96.6093\n\nEpoch 64/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26283.1504 - mae: 106.9620 - val_loss: 22970.6426 - val_mae: 97.3550\n\nEpoch 65/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26414.2891 - mae: 107.2428 - val_loss: 22562.5410 - val_mae: 95.9161\n\nEpoch 66/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26236.5078 - mae: 106.8431 - val_loss: 22565.6289 - val_mae: 96.3522\n\nEpoch 67/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26235.1191 - mae: 106.7647 - val_loss: 22878.8359 - val_mae: 96.8295\n\nEpoch 68/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 26190.4219 - mae: 106.9940 - val_loss: 22628.1211 - val_mae: 96.4429\n\nEpoch 69/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 25758.3574 - mae: 105.8355 - val_loss: 23017.6055 - val_mae: 96.9442\n\nEpoch 70/200\n\n308/308 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 25868.6289 - mae: 106.3213 - val_loss: 22637.5410 - val_mae: 96.3798\n\n\n\nTest MSE: 22550.83\n\nTest MAE: 97.27 riders\n\n528/528 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 2ms/step\n\nTest R¬≤:  0.8117\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDone.\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\n\n# -----------------------------\n# 1) Load holdout\n# -----------------------------\nHOLDOUT_URL = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv\"\nholdout = pd.read_csv(HOLDOUT_URL)\n\n# -----------------------------\n# 2) Apply same transformations\n#    (match what you did for bike)\n# -----------------------------\nholdout[\"dteday\"] = pd.to_datetime(holdout[\"dteday\"])\nholdout[\"day_of_week\"] = holdout[\"dteday\"].dt.day_name()\n\nday_map = {\n    \"Monday\": 0, \"Tuesday\": 1, \"Wednesday\": 2, \"Thursday\": 3,\n    \"Friday\": 4, \"Saturday\": 5, \"Sunday\": 6\n}\nholdout[\"day_of_week_num\"] = holdout[\"day_of_week\"].map(day_map)\n\n# --- Event flags (use the SAME windows you used above) ---\n# covid_lockdown\ncovid_lockdown_ranges = [(\"2020-04-01\", \"2020-05-29\")]  # as in your earlier cell [page:1]\nholdout[\"covid_lockdown\"] = 0\nfor start, end in covid_lockdown_ranges:\n    start = pd.to_datetime(start); end = pd.to_datetime(end)\n    holdout.loc[(holdout[\"dteday\"] &gt;= start) & (holdout[\"dteday\"] &lt;= end), \"covid_lockdown\"] = 1\n\n# gov_shutdown\ngov_shutdown_ranges = [(\"2013-10-01\",\"2013-10-17\"), (\"2018-01-20\",\"2018-01-22\"), (\"2018-12-22\",\"2019-01-25\")]\nholdout[\"gov_shutdown\"] = 0\nfor start, end in gov_shutdown_ranges:\n    start = pd.to_datetime(start); end = pd.to_datetime(end)\n    holdout.loc[(holdout[\"dteday\"] &gt;= start) & (holdout[\"dteday\"] &lt;= end), \"gov_shutdown\"] = 1\n\n# big_event\nbig_event_ranges = [(\"2013-01-18\",\"2013-01-22\"), (\"2017-01-19\",\"2017-01-21\"), (\"2021-01-19\",\"2021-01-21\"),\n                    (\"2015-09-22\",\"2015-09-24\"), (\"2020-06-01\",\"2020-06-02\")]\nholdout[\"big_event\"] = 0\nfor start, end in big_event_ranges:\n    start = pd.to_datetime(start); end = pd.to_datetime(end)\n    holdout.loc[(holdout[\"dteday\"] &gt;= start) & (holdout[\"dteday\"] &lt;= end), \"big_event\"] = 1\n\n# major_holiday (same logic as your notebook: US federal holidays + fixed dates, and uses existing holiday col) [page:1]\nmin_day = holdout[\"dteday\"].min()\nmax_day = holdout[\"dteday\"].max()\ncal = USFederalHolidayCalendar()\nfederal_holidays = cal.holidays(start=min_day, end=max_day)\n\nfixed_major_md = {(1, 1), (7, 4), (12, 25)}\nfixed_major = holdout[\"dteday\"].apply(lambda d: (d.month, d.day) in fixed_major_md)\n\nholdout[\"major_holiday\"] = (\n    (((holdout[\"holiday\"] == 1) & (holdout[\"dteday\"].isin(federal_holidays) | fixed_major)) | fixed_major)\n).astype(int)\n\n# IMPORTANT: your model expects weather_emergency too (it was in feature_cols) [page:1]\n# Your notebook printed the flagged dates list and created the column there; replicate by\n# defaulting to 0 unless you have a specific rule cell to copy over.\nif \"weather_emergency\" not in holdout.columns:\n    holdout[\"weather_emergency\"] = 0\n\n# -----------------------------\n# 3) Build X in the same column order and scale with existing scaler\n# -----------------------------\nX_holdout = holdout[feature_cols].values   # feature_cols defined in your training cell [page:1]\nX_holdout_scaled = scaler.transform(X_holdout)  # scaler already fit on training data [page:1]\n\n# -----------------------------\n# 4) Predict + save exactly as required\n# -----------------------------\npred = model.predict(X_holdout_scaled).reshape(-1)\n\nsubmission = pd.DataFrame({\"predictions\": pred})\nsubmission.to_csv(\"team4-module4-predictions.csv\", index=False)\n\nsubmission.head(), submission.shape\n\n\n12/12 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 3ms/step \n\n\n\n\n(   predictions\n 0          0.0\n 1          0.0\n 2          0.0\n 3          0.0\n 4          0.0,\n (384, 1))"
  },
  {
    "objectID": "notebooks/starter_bank.html",
    "href": "notebooks/starter_bank.html",
    "title": "Jacob Walker - Data Science Portfolio",
    "section": "",
    "text": "import pandas as pd\n\ncampaign = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bank.csv')\n\ncampaign\n\n\n    \n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\ncampaign\npdays\nprevious\npoutcome\nemp.var.rate\ncons.price.idx\ncons.conf.idx\neuribor3m\nnr.employed\ny\n\n\n\n\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\nmon\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\nmon\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\nmon\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\nmon\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\nmon\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n37064\n73\nretired\nmarried\nprofessional.course\nno\nyes\nno\ncellular\nnov\nfri\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nyes\n\n\n37065\n46\nblue-collar\nmarried\nprofessional.course\nno\nno\nno\ncellular\nnov\nfri\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n37066\n56\nretired\nmarried\nuniversity.degree\nno\nyes\nno\ncellular\nnov\nfri\n2\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n37067\n44\ntechnician\nmarried\nprofessional.course\nno\nno\nno\ncellular\nnov\nfri\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nyes\n\n\n37068\n74\nretired\nmarried\nprofessional.course\nno\nyes\nno\ncellular\nnov\nfri\n3\n999\n1\nfailure\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n\n\n37069 rows √ó 20 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Reload clean copy of the data (or use your existing 'campaign' if it is untouched)\ncampaign = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bank.csv')\n\n# Drop obvious missing rows (this dataset has very few)\ncampaign = campaign.dropna().copy()\n\ncampaign.head()\ncampaign['y'].value_counts(normalize=True)\n\n\n\n\n\n\n\n\nproportion\n\n\ny\n\n\n\n\n\nno\n0.886482\n\n\nyes\n0.113518\n\n\n\n\ndtype: float64\n\n\n\n# Target: did the client subscribe to a term deposit?\ny = campaign['y'].map({'no': 0, 'yes': 1})   # convert to 0/1\n\n# Drop target to get features\nX = campaign.drop(columns=['y'])\n\n# Identify numeric vs categorical columns\nnumeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n\nnumeric_cols, categorical_cols\n\n(['age',\n  'campaign',\n  'pdays',\n  'previous',\n  'emp.var.rate',\n  'cons.price.idx',\n  'cons.conf.idx',\n  'euribor3m',\n  'nr.employed'],\n ['job',\n  'marital',\n  'education',\n  'default',\n  'housing',\n  'loan',\n  'contact',\n  'month',\n  'day_of_week',\n  'poutcome'])\n\n\n\n# One‚Äëhot encode categoricals, leave numerics as-is\npreprocess = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n        ('num', 'passthrough', numeric_cols)\n    ]\n)\n\nrf = RandomForestClassifier(\n    n_estimators=300,\n    max_depth=None,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    random_state=42,\n    n_jobs=-1,\n    class_weight='balanced'  # very important: target is imbalanced\n)\n\nmodel = Pipeline(steps=[\n    ('preprocess', preprocess),\n    ('rf', rf)\n])\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.25,\n    random_state=42,\n    stratify=y\n)\n\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['job', 'marital',\n                                                   'education', 'default',\n                                                   'housing', 'loan', 'contact',\n                                                   'month', 'day_of_week',\n                                                   'poutcome']),\n                                                 ('num', 'passthrough',\n                                                  ['age', 'campaign', 'pdays',\n                                                   'previous', 'emp.var.rate',\n                                                   'cons.price.idx',\n                                                   'cons.conf.idx', 'euribor3m',\n                                                   'nr.employed'])])),\n                ('rf',\n                 RandomForestClassifier(class_weight='balanced',\n                                        min_samples_leaf=5,\n                                        min_samples_split=10, n_estimators=300,\n                                        n_jobs=-1, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['job', 'marital',\n                                                   'education', 'default',\n                                                   'housing', 'loan', 'contact',\n                                                   'month', 'day_of_week',\n                                                   'poutcome']),\n                                                 ('num', 'passthrough',\n                                                  ['age', 'campaign', 'pdays',\n                                                   'previous', 'emp.var.rate',\n                                                   'cons.price.idx',\n                                                   'cons.conf.idx', 'euribor3m',\n                                                   'nr.employed'])])),\n                ('rf',\n                 RandomForestClassifier(class_weight='balanced',\n                                        min_samples_leaf=5,\n                                        min_samples_split=10, n_estimators=300,\n                                        n_jobs=-1, random_state=42))]) preprocess: ColumnTransformer?Documentation for preprocess: ColumnTransformerColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['job', 'marital', 'education', 'default',\n                                  'housing', 'loan', 'contact', 'month',\n                                  'day_of_week', 'poutcome']),\n                                ('num', 'passthrough',\n                                 ['age', 'campaign', 'pdays', 'previous',\n                                  'emp.var.rate', 'cons.price.idx',\n                                  'cons.conf.idx', 'euribor3m',\n                                  'nr.employed'])]) cat['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') num['age', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'] passthroughpassthrough RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(class_weight='balanced', min_samples_leaf=5,\n                       min_samples_split=10, n_estimators=300, n_jobs=-1,\n                       random_state=42) \n\n\n\ny_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred, digits=3))\n\n[[7417  799]\n [ 414  638]]\n              precision    recall  f1-score   support\n\n           0      0.947     0.903     0.924      8216\n           1      0.444     0.606     0.513      1052\n\n    accuracy                          0.869      9268\n   macro avg      0.696     0.755     0.719      9268\nweighted avg      0.890     0.869     0.878      9268\n\n\n\n\n# Get feature names after one‚Äëhot encoding\nohe = model.named_steps['preprocess'].named_transformers_['cat']\nohe_features = ohe.get_feature_names_out(categorical_cols)\nall_features = list(ohe_features) + numeric_cols\n\nimportances = model.named_steps['rf'].feature_importances_\n\nfeat_imp = pd.DataFrame({\n    'feature': all_features,\n    'importance': importances\n}).sort_values('importance', ascending=False)\n\nfeat_imp.head(20)\n\n\n    \n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n60\neuribor3m\n0.145208\n\n\n61\nnr.employed\n0.114732\n\n\n53\nage\n0.088984\n\n\n57\nemp.var.rate\n0.087936\n\n\n54\ncampaign\n0.049006\n\n\n59\ncons.conf.idx\n0.041911\n\n\n58\ncons.price.idx\n0.036195\n\n\n55\npdays\n0.027286\n\n\n52\npoutcome_success\n0.021230\n\n\n34\ncontact_telephone\n0.018355\n\n\n41\nmonth_may\n0.017552\n\n\n33\ncontact_cellular\n0.014141\n\n\n46\nday_of_week_mon\n0.013581\n\n\n29\nhousing_yes\n0.013200\n\n\n56\nprevious\n0.013061\n\n\n27\nhousing_no\n0.013046\n\n\n0\njob_admin.\n0.012512\n\n\n51\npoutcome_nonexistent\n0.012191\n\n\n22\neducation_university.degree\n0.012175\n\n\n13\nmarital_married\n0.012103\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 1"
    ]
  }
]